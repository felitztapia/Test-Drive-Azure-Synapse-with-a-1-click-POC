{
	"name": "DI_02_Get_Schema",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "felitztapia",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6f4ff107-5903-4ae3-9f22-b084432e0d63"
			}
		},
		"metadata": {
			"saveOutput": false,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
				"name": "felitztapia",
				"type": "Spark",
				"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Metadata-Driven Ingestion Framework \r\n",
					"#### Data Ingestion: Get Schemas\r\n",
					"Connect to sink instance and get the schema of the columns of parquet file. Validate the result and send it to Azure Data Factory."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### IMPORTANT!\r\n",
					"#### Configuration for testing and debug\r\n",
					"Change the value of \"testing=False\" for production environment.\r\n",
					"Change the value of debug variables to see or hide prints with information."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
					"print_dictionaries = False\r\n",
					"print_common_variables = False\r\n",
					"print_empty_variables = False\r\n",
					"\r\n",
					"#SinkGlobalParameters = \"{\\\"kv_scope_name\\\":\\\"KeyVaultSecrets\\\",\\\"kv_workspace_id\\\":\\\"MDIF-la-workspace-Id\\\",\\\"kv_workspace_pk\\\":\\\"MDIF-la-workspace-pk\\\",\\\"raw_storage_name\\\":\\\"\\\",\\\"raw_storage_secret_name\\\":\\\"\\\",\\\"sink_container_name\\\":\\\"sink\\\",\\\"schema_container_name\\\":\\\"schemas\\\",\\\"output_container_name\\\":\\\"datatransformation\\\",\\\"adls_storage_name\\\":\\\"adlsmetadatadriven2\\\",\\\"adls_storage_secret_name\\\":\\\"MDIF-ADLSmetadatadriven2-AccountKey\\\",\\\"sink_type\\\":\\\"ADLS\\\"}\"\r\n",
					"\r\n",
					"#FwkParameters = \"{\\\"FwkSourceId\\\":1,\\\"SourceType\\\":\\\"ADLS\\\",\\\"FwkConfigId\\\":4,\\\"SrcObject\\\":\\\"books1.xml\\\",\\\"SchemaName\\\":\\\"fileXML\\\",\\\"DatabaseName\\\":null,\\\"WmkColumnName\\\":null,\\\"WmkDataType\\\":0,\\\"TypeLoad\\\":1,\\\"RelativeURL\\\":null,\\\"ActiveFlag\\\":\\\"Y\\\",\\\"Header01\\\":null,\\\"Header02\\\":null,\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/xmlsample\\\",\\\"SinkPathGranularity\\\":\\\"HH\\\",\\\"IPAddress\\\":null,\\\"FwkTriggerId\\\":3,\\\"BatchGroupId\\\":-1,\\\"ConvertMethod\\\":\\\"Databricks\\\",\\\"SynapseObject\\\":\\\"ExternalView\\\"}\"\r\n",
					"\r\n",
					"ConvertedPath = \"Converted/ADLS/fileXML/books1/2022/01/11/23/\"\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#import the necessary libraries\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"import json"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### Declaration of variables and execution of functions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Obtain the parameters sent by Azure Data Factory\r\n",
					"SinkGlobalParameters=''\r\n",
					"#sink_params = dbutils.widgets.get(\"SinkGlobalParameters\") if testing==False else SinkGlobalParameters\r\n",
					"\r\n",
					"FwkParameters=''\r\n",
					"#fwk_params = dbutils.widgets.get(\"FwkParameters\") if testing==False else FwkParameters\r\n",
					"\r\n",
					"ConvertedPath=''\r\n",
					"#file_name = dbutils.widgets.get(\"ConvertedPath\") if testing==False else ConvertedPath"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Obtain the parameters sent by Azure Data Factory\r\n",
					"sink_params = SinkGlobalParameters\r\n",
					"\r\n",
					"fwk_params = FwkParameters\r\n",
					"\r\n",
					"file_name = ConvertedPath"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Convert the string from ADF to a dictionaries using utilities.\r\n",
					"fwk_params_dict = json.loads(fwk_params)\r\n",
					"sink_params_dict = json.loads(sink_params)\r\n",
					"#fwk_params_dict"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Name of the Azure Key Vault-backed scope\r\n",
					"fwklog_id = fwk_params_dict[\"FwkSourceId\"]  \r\n",
					"#function_name = str(fwk_params_dict[\"FunctionName\"]).strip()                           # Validation Function name\r\n",
					"function_name = \"DI Get Schema\"\r\n",
					"cvt_method = str(fwk_params_dict[\"ConvertMethod\"]).strip()                                   # Data validation method name\r\n",
					"\r\n",
					"\r\n",
					"kv_scope_name = sink_params_dict[\"kv_scope_name\"]                                     # Name of the Azure Key Vault-backed scope\r\n",
					"kv_workspace_id = sink_params_dict[\"kv_workspace_id\"].strip()                         # Name of the secret for the log analytics workspace id\r\n",
					"kv_workspace_pk = sink_params_dict[\"kv_workspace_pk\"].strip()                         # Name of the secret for the log analytics primary key\r\n",
					"\r\n",
					"# Name of the container in the Azure ADLS Storage Account (sink)\r\n",
					"#sink_container_name = 'sink'\r\n",
					"sink_container_name=sink_params_dict['sink_container_name']\r\n",
					"# Name of the Azure data lake gen 2 \r\n",
					"adls_storage_account_name = sink_params_dict[\"adls_storage_name\"]    \r\n",
					"\r\n",
					"# Name of the container in the Azure data lake\r\n",
					"adls_blob_secret_name = sink_params_dict[\"adls_storage_secret_name\"]            "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql.functions import *\r\n",
					"#Declare source variables\r\n",
					"\r\n",
					"source_path = 'taxidata/Date/dbo.Date.parquet'\r\n",
					"#replace // to get the correct path \r\n",
					"source_path = source_path.replace('//', '/')\r\n",
					"#adls_source_name = fwk_params_dict['InstanceURL'].replace('https://','')\r\n",
					"adls_storage_account_name='ftr23m5xeeoklumapoc'\r\n",
					"\r\n",
					"adls_source_name = adls_storage_account_name + '.dfs.core.windows.net/'\r\n",
					"print(adls_source_name, \"|\",source_path)\r\n",
					"\r\n",
					"#adls_source_secret = fwk_params_dict['SecretName']\r\n",
					"source_container_name = 'dlsftrpocfs1'\r\n",
					"#source_container_name=sink_container_name\r\n",
					"kv_scope_name='ADLS_spark'"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql.functions import *\r\n",
					"\r\n",
					"#SPARK CONFIGURATIONS\r\n",
					"print(\"Start process\")\r\n",
					"#Set up an account access keySet up an account access key\r\n",
					"spark.conf.set(\"spark.storage.synapse.linkedServiceName\", kv_scope_name)\r\n",
					"spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\r\n",
					"\r\n",
					"#spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\r\n",
					"#+spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Read parquet files from adls path as a dataframe\r\n",
					"path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
					"#path += '*.parquet'\r\n",
					"print(f\"reading file from: {path}\")\r\n",
					"df = spark.read.parquet(path) #all parquet files\r\n",
					"for colu in df.columns:\r\n",
					"  df = df.withColumn(colu, df[colu].cast(StringType())) #convert it to string first\r\n",
					"\r\n",
					"\r\n",
					"df.count()\r\n",
					"#display(df)\r\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Class that will evaluate cast for specific column of each datatype \r\n",
					"\r\n",
					"\r\n",
					"class DTypecheck: # it works for pyspark dataframe only\r\n",
					"  \r\n",
					"  def __init__(self, dataframe):\r\n",
					"  #when class is initialized store dataframe to shorter names\r\n",
					"      import dateutil\r\n",
					"      self.df=dataframe\r\n",
					"  \r\n",
					"  @classmethod\r\n",
					"  def from_pdf(cls,pdf): #this is if we have a pandas dataframe it should run like DTypecheck.from_pdf(nameofpandasdf).get_schema()\r\n",
					"    df=spark.createDataFrame(pdf) \r\n",
					"    return cls(df)\r\n",
					"  \r\n",
					"  def filterna(self,c):\r\n",
					"    \r\n",
					"    #filter 'nulls' of the column if needed (it slows down code)\r\n",
					"    if (self.df.filter((self.df[c] == \"NULL\"))).count()>0: #if there are null values present in column\r\n",
					"      self.df =(self.df.filter((self.df[c] == \"NULL\")))   #filter column nulls \r\n",
					"    # if (self.df.filter((self.df[c] == \" \"))).count()>0: #if there are null values present in column\r\n",
					"    #   self.df =(self.df.filter((self.df[c] == \" \")))   #filter column nulls \r\n",
					"    if (self.df.filter((self.df[c] == \"\"))).count()>0: #if there are null values present in column\r\n",
					"      self.df =(self.df.filter((self.df[c] == \"\")))   #filter column nulls \r\n",
					"    #  if (self.df.filter((self.df[c] == None))).count()>0: #if there are null values present in column\r\n",
					"    # self.df =(self.df.filter((self.df[c] == None)))   #filter column nulls    \r\n",
					"    dfna=self.df\r\n",
					"    return dfna\r\n",
					"  \r\n",
					"  \r\n",
					"  def bit(self,column2evaluate): #function to evaluate if values are boolean, 1st function to evaluate\r\n",
					"    \r\n",
					"    self.col=column2evaluate\r\n",
					"    df2=self.df\r\n",
					"    \r\n",
					"    #if 1s or 0s replace them with none bc it will think that it is a True or False when casting\r\n",
					"    df2 = df2.withColumn(self.col, when(df2[self.col] == '1', None).otherwise(df2[self.col]))\r\n",
					"    df2 = df2.withColumn(self.col, when(df2[self.col] == '0', None).otherwise(df2[self.col]))\r\n",
					"    \r\n",
					"    df3 = df2.withColumn(self.col, df2[self.col].cast(BooleanType())) #perform casting\r\n",
					"    if df3.filter(df3[self.col].isNull()).count()>=0.7*(df3.count()): #if more than 70% are null values then casting failed and col is not bit\r\n",
					"      return 0\r\n",
					"    else:\r\n",
					"      return 1\r\n",
					"    \r\n",
					"  def integer(self,column2evaluate): # use this function to validate for int if and only if bit failed, 2nd funct to evaluate \r\n",
					"    \r\n",
					"        self.col=column2evaluate\r\n",
					"        #df2=self.filterna(self.col) #check for null values to avoid counting more nulls\r\n",
					"        df2=self.df\r\n",
					"        df3 = df2.withColumn(self.col, round(df2[self.col]).cast(IntegerType())) #round up to make sure first they are numbers\r\n",
					"        if df3.filter(df3[self.col].isNull()).count()>=0.7*(df3.count()): #if more than 70% are nulls then its sure not an integer\r\n",
					"          return 0\r\n",
					"        else:\r\n",
					"          return 1\r\n",
					"\r\n",
					"  def decimal(self,column2evaluate): #this funct should only be called if integer returned 1, 3rd function to evaluate\r\n",
					"      self.col=column2evaluate\r\n",
					"      if (self.df.filter(self.df[self.col].contains('.'))).count()==0: #if integer was validated and it contains . is decimal\r\n",
					"        return 0\r\n",
					"      else:\r\n",
					"        return 1\r\n",
					"        \r\n",
					"  def biginteger(self,column2evaluate): #this function should be called ig integer is validated and decimal is not 4th funct to evaluate\r\n",
					"    \r\n",
					"      self.col=column2evaluate\r\n",
					"      df3 = self.df.withColumn(self.col, round(self.df[self.col]).cast(IntegerType())) #round up to make sure first they are numbers\r\n",
					"\r\n",
					"      if df3.where(df3[self.col]==2147483647).count()>0: #check if there is a value higher than the largest int it will set it to max int\r\n",
					"        return 1\r\n",
					"      else:\r\n",
					"        return 0\r\n",
					"\r\n",
					"  def date(self,column2evaluate): #if integer returned 0, bit must've returned 0 too, so we try date 5th function to evaluate\r\n",
					"    \r\n",
					"      import dateutil\r\n",
					"      self.col=column2evaluate\r\n",
					"      #self.df=self.filterna(self.col) \r\n",
					"      cont=0\r\n",
					"      for el in self.df.select(self.col).collect(): #iterate over each value of the columns\r\n",
					"        try: \r\n",
					"          dateutil.parser.parse(el[0]) #try to parse it to date\r\n",
					"        except:\r\n",
					"          cont+=1 #if you cant parse it to date store the issue \r\n",
					"      if cont>=0.7*self.df.select(self.col).count():#if more than 70% of the data cant be converted to datetime this col shouldn't be datetime\r\n",
					"        return 0\r\n",
					"      else:\r\n",
					"        return 1\r\n",
					"\r\n",
					"  def datetime(self,column2evaluate): #this func should only be called after validating that date is 1 \r\n",
					"    \r\n",
					"      import dateutil\r\n",
					"      self.col=column2evaluate\r\n",
					"      contv=0\r\n",
					"      for el in self.df.select(self.col).collect():\r\n",
					"        try:\r\n",
					"          val=(dateutil.parser.parse(el[0])) #we know that data can be parsed since date returned 1\r\n",
					"          try:\r\n",
					"            if val.hour>0 or val.minute>0: #if at least 1 value has hour or minute set to more than 1 its datetime\r\n",
					"              contv+=1\r\n",
					"          except:\r\n",
					"            pass\r\n",
					"        except:#if it cant be parsed it is an empty string so we just ignore it\r\n",
					"          pass\r\n",
					"      if contv>0: #if we registered that at least one element that is datetime has hours then we use datetime not date\r\n",
					"        return 1\r\n",
					"      else:\r\n",
					"        return 0\r\n",
					"      \r\n",
					"  def maxvarchar(self,column2evaluate): #we should use this function only after validating it it is not anything else \r\n",
					"    #this function will be evaluated second to last\r\n",
					"      self.col=column2evaluate\r\n",
					"      if self.df.where(length(col(self.col)) >255).count()>0: #if we have at least one value higher than 255 use var char max\r\n",
					"        return 1\r\n",
					"      else:\r\n",
					"        return 0\r\n",
					"      \r\n",
					"  def varchar(self,column2evaluate):\r\n",
					"    #this function should be evaluated last\r\n",
					"    self.col=column2evaluate\r\n",
					"    if self.maxvarchar(self.col)==0:\r\n",
					"      return 1\r\n",
					"    \r\n",
					"  def get_schema(self):\r\n",
					"    \r\n",
					"    schema = []\r\n",
					"    cols = self.df.columns\r\n",
					"    \r\n",
					"    for n,c in enumerate(cols, 1): #Main logic to validate schemas for each column\r\n",
					"      \r\n",
					"      data = {}\r\n",
					"      data['id'] = str(n)\r\n",
					"      data['name'] = c\r\n",
					"      #check if column is boolean\r\n",
					"      if self.bit(c) == 1:\r\n",
					"        data['type']='Bit'\r\n",
					"       # print(f'Column {c} is Boolean')\r\n",
					"      elif self.integer(c) == 1:\r\n",
					"        if self.decimal(c) == 1: #now we know this column is a number so we can check for .\r\n",
					"          data['type']='Decimal(18,4)'\r\n",
					"          #print(f'Column {c} is a decimal number')\r\n",
					"\r\n",
					"        elif self.biginteger(c) == 1: #if it is not a decimal it still may be a big integer\r\n",
					"          data['type']='Bigint'\r\n",
					"          #print(f'Column {c} is a biginteger')\r\n",
					"          \r\n",
					"        else: #if it is not one of those then it is just an integer\r\n",
					"          data['type'] = 'Int'\r\n",
					"          #print(f'Column {c} is an integer') \r\n",
					"      elif self.date(c) == 1: #if it is not an integer check it can be parsed to date \r\n",
					"        if self.datetime(c) == 1:\r\n",
					"          data['type'] = 'Datetime'\r\n",
					"          #print(f'Column {c} is datetime')\r\n",
					"        else:\r\n",
					"          data['type'] = 'Date'\r\n",
					"          #print(f'Column {c} is a date')\r\n",
					"      else: #it should definitely be a string\r\n",
					"        if self.maxvarchar(c) == 1:\r\n",
					"          data['type']='Varchar (max)'\r\n",
					"          #print(f'Column {c} is a maxvarchar')\r\n",
					"        else: #if its not any of those it should be this one\r\n",
					"            data['type']='Varchar (255)'\r\n",
					"            #print(f'Column {c} is of varchar 255')\r\n",
					"            \r\n",
					"      schema.append(data)\r\n",
					"      \r\n",
					"    return schema\r\n",
					"              \r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"jsonschema = DTypecheck(df).get_schema() #give the dataframe input \r\n",
					"\r\n",
					"jsonschema"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"mssparkutils.notebook.exit(str(jsonschema))"
				],
				"execution_count": null
			}
		]
	}
}