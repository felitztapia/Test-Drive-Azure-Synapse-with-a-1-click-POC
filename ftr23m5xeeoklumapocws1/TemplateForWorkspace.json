{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "ftr23m5xeeoklumapocws1"
		},
		"ADLS_Spark_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'ADLS_Spark'"
		},
		"TripFaresSynapseAnalyticsLinkedService_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'TripFaresSynapseAnalyticsLinkedService'"
		},
		"ADLS_Spark_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://aicscpgdemo02.dfs.core.windows.net/"
		},
		"AzureKeyVaultSpark_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://kvftr23m5xeeoklumapoc.vault.azure.net/"
		},
		"HttpServerTripDataLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/tripDataAndFaresCSV/trip-data.csv"
		},
		"HttpServerTripFareDataLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/tripDataAndFaresCSV/fares-data.csv"
		},
		"LS_adls_sink_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://aicscpgdemo02.dfs.core.windows.net/"
		},
		"OpdAdls_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://pocsynapseadlseum.dfs.core.windows.net"
		},
		"TripFaresDataLakeStorageLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().datalakeAccountName,'.dfs.core.windows.net')}"
		},
		"keyVaultLinkedservice_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().keyVaultName,'.vault.azure.net/')}"
		},
		"nyc_tlc_yellow_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'nyc_tlc_yellow'"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/felitztapia')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Extract Data From SQL Pool')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy License",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlPoolSource",
								"queryTimeout": "02:00:00"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LS_SQLPoolHackneyLicense",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ParquetHAckneyLicense",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Copy Weather",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlPoolSource",
								"queryTimeout": "02:00:00"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LS_SQLPoolWeather",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ParquetWeather",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Copy Time",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlPoolSource",
								"queryTimeout": "02:00:00"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LS_SQLPoolTime",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ParquetTime",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Copy Geography",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlPoolSource",
								"queryTimeout": "02:00:00"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LS_SQLPoolGeography",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ParquetGeography",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Copy Trip",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlPoolSource",
								"queryTimeout": "02:00:00"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LS_SQLPoolTrip",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ParquetTrip",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Copy Medallion",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlPoolSource",
								"queryTimeout": "02:00:00"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LS_SQLPoolMedallion",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ParquetMEdallion",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Copy Date",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlPoolSource",
								"queryTimeout": "02:00:00"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LS_SQLPoolDate",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ParquetDate",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "TripFaresDataPipeline"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/LS_SQLPoolHackneyLicense')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ParquetHAckneyLicense')]",
				"[concat(variables('workspaceId'), '/datasets/LS_SQLPoolWeather')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ParquetWeather')]",
				"[concat(variables('workspaceId'), '/datasets/LS_SQLPoolTime')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ParquetTime')]",
				"[concat(variables('workspaceId'), '/datasets/LS_SQLPoolGeography')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ParquetGeography')]",
				"[concat(variables('workspaceId'), '/datasets/LS_SQLPoolTrip')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ParquetTrip')]",
				"[concat(variables('workspaceId'), '/datasets/LS_SQLPoolMedallion')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ParquetMEdallion')]",
				"[concat(variables('workspaceId'), '/datasets/LS_SQLPoolDate')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ParquetDate')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Extract Trip Data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy Trip",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlPoolSource",
								"sqlReaderQuery": {
									"value": "@concat('SELECT HASHBYTES(''SHA1'',concat([DateID],[MedallionID], [HackneyLicenseID], [PickupTimeID])) AS TripId, *,getdate()-', pipeline().parameters.DaysBack, ' as updatedtimestamp from trip where dateid = ',pipeline().parameters.Date)",
									"type": "Expression"
								},
								"queryTimeout": "02:00:00"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LS_SQLPoolTrip",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ParquetTrip_ByDay",
								"type": "DatasetReference",
								"parameters": {
									"DateId": {
										"value": "@pipeline().parameters.Date",
										"type": "Expression"
									},
									"keyVaultName": "kvftr23m5xeeoklumapoc"
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"Date": {
						"type": "string",
						"defaultValue": "20130101"
					},
					"DaysBack": {
						"type": "string",
						"defaultValue": "3"
					}
				},
				"folder": {
					"name": "TripFaresDataPipeline"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/LS_SQLPoolTrip')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ParquetTrip_ByDay')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Extract Trip Updated Data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy Trip",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlPoolSource",
								"sqlReaderQuery": "SELECT * FROM UpdatedTripData",
								"queryTimeout": "02:00:00"
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LS_SQLPoolTrip",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DS_ParquetTrip_ByDay",
								"type": "DatasetReference",
								"parameters": {
									"DateId": {
										"value": "@pipeline().parameters.Date",
										"type": "Expression"
									},
									"keyVaultName": "kvftr23m5xeeoklumapoc"
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"Date": {
						"type": "string",
						"defaultValue": "20211122"
					}
				},
				"folder": {
					"name": "TripFaresDataPipeline"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/LS_SQLPoolTrip')]",
				"[concat(variables('workspaceId'), '/datasets/DS_ParquetTrip_ByDay')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoopDates')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "ForEachDate",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@createArray('20130106', '20130107','20130108','20130109','20130110')",
								"type": "Expression"
							},
							"isSequential": true,
							"activities": [
								{
									"name": "Call Extraction",
									"type": "ExecutePipeline",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"pipeline": {
											"referenceName": "Extract Trip Data",
											"type": "PipelineReference"
										},
										"waitOnCompletion": true,
										"parameters": {
											"Date": {
												"value": "@item()",
												"type": "Expression"
											}
										}
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"PartitionDate": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Extract Trip Data')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PL-AutoPause-Resume-SQLPool')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Automatically pause and resume the Dedicated SQL Pool based on the schedules defined in the triggers.",
				"activities": [
					{
						"name": "Determine State",
						"description": "Check whether to Pause or Resume the Dedicated SQL Pool",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "GetPoolState",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@if(equals(activity('GetPoolState').output.properties.status,'Online'),'True','False')",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "Pause",
									"description": "REST API call to pause the Dedicated SQL Pool",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": "https://management.azure.com/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/sqlPools/ftr23m5xeeoklumapocws1p1/pause?api-version=2021-05-01",
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "POST",
										"headers": {},
										"body": "{}",
										"authentication": {
											"type": "MSI",
											"resource": "https://management.azure.com/"
										}
									}
								}
							]
						}
					},
					{
						"name": "GetPoolState",
						"description": "validate the status of the sql pool",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://management.azure.com/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/sqlPools/ftr23m5xeeoklumapocws1p1?api-version=2019-06-01-preview')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://management.azure.com/"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": [],
				"lastPublishTime": "2021-11-10T23:53:50Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TestingRowCount')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Notebook1",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "MDMF_RowCount",
								"type": "NotebookReference"
							},
							"parameters": {
								"SinkGlobalParameters": {
									"value": "Val1",
									"type": "string"
								},
								"DataValidationParameters": {
									"value": "Val2",
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "felitztapia",
								"type": "BigDataPoolReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "MDMF_Validation"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/MDMF_RowCount')]",
				"[concat(variables('workspaceId'), '/bigDataPools/felitztapia')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresDataPipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "IngestTripDataIntoADLS",
						"description": "Copies the trip data csv file from the git repo and loads it into the ADLS.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.00:10:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "tripsDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "tripDataSink",
								"type": "DatasetReference",
								"parameters": {
									"datalakeAccountName": {
										"value": "@pipeline().parameters.datalakeAccountName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "IngestTripFaresDataIntoADLS",
						"description": "Copies the trip fare data csv file from the git repo and loads it into the ADLS.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.00:10:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "faresDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "faresDataSink",
								"type": "DatasetReference",
								"parameters": {
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"datalakeAccountName": {
										"value": "@pipeline().parameters.datalakeAccountName",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "JoinAndAggregateData",
						"description": "Reads the raw data from both CSV files inside the ADLS, performs the desired transformations (inner join and aggregation) and writes the transformed data into the synapse SQL pool.",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.00:30:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "tripFaresDataTransformations",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"TripDataCSV": {
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										},
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										}
									},
									"FaresDataCSV": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									},
									"SynapseAnalyticsSink": {
										"SchemaName": {
											"value": "@pipeline().parameters.SchemaName",
											"type": "Expression"
										},
										"SynapseWorkspaceName": {
											"value": "@pipeline().parameters.SynapseWorkspaceName",
											"type": "Expression"
										},
										"SQLDedicatedPoolName": {
											"value": "@pipeline().parameters.SQLDedicatedPoolName",
											"type": "Expression"
										},
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"SQLLoginUsername": {
											"value": "@pipeline().parameters.SQLLoginUsername",
											"type": "Expression"
										}
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Create Schema If Does Not Exists",
						"description": "Creates the schema inside the SQL dedicated pool. Shema name comes from the pipeline parameter 'SchemaName'.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "IngestTripDataIntoADLS",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "IngestTripFaresDataIntoADLS",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.00:05:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = '@{pipeline().parameters.SchemaName}')\nBEGIN\nEXEC('CREATE SCHEMA @{pipeline().parameters.SchemaName}')\nselect Count(*) from sys.symmetric_keys;\nEND\nELSE\nBEGIN\n    select Count(*) from sys.symmetric_keys;\nEND",
									"type": "Expression"
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "azureSynapseAnalyticsSchema",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Copy data Trips Data",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "IF (EXISTS (SELECT *\n  FROM INFORMATION_SCHEMA.TABLES\n  WHERE TABLE_SCHEMA = 'dbo'\n  AND TABLE_NAME = 'TripsData'))\nBEGIN \n   Truncate table TripsData;\nEnd\n",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"disableMetricsCollection": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "TripFaresDataLakeStorageLinkedService",
									"type": "LinkedServiceReference",
									"parameters": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									}
								}
							}
						},
						"inputs": [
							{
								"referenceName": "tripsDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "AzureSynapseAnalyticsTripsData",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "Copy data Fares Data",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "IF (EXISTS (SELECT *\n  FROM INFORMATION_SCHEMA.TABLES\n  WHERE TABLE_SCHEMA = 'dbo'\n  AND TABLE_NAME = 'FaresData'))\nBEGIN \n   Truncate table FaresData;\nEnd\n",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"disableMetricsCollection": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "TripFaresDataLakeStorageLinkedService",
									"type": "LinkedServiceReference",
									"parameters": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									}
								}
							}
						},
						"inputs": [
							{
								"referenceName": "faresDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "AzureSynapseAnalyticsFaresData",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"SchemaName": {
						"type": "string",
						"defaultValue": "tripFares"
					},
					"SynapseWorkspaceName": {
						"type": "string",
						"defaultValue": "<synapse-workspace-name>.database.windows.net"
					},
					"SQLDedicatedPoolName": {
						"type": "string",
						"defaultValue": "<sql-dedicated-pool-name>"
					},
					"SQLLoginUsername": {
						"type": "string",
						"defaultValue": "<sql-login-username>"
					},
					"KeyVaultName": {
						"type": "string",
						"defaultValue": "<keyvaukt-name>"
					},
					"datalakeAccountName": {
						"type": "string",
						"defaultValue": "<datalake-account-name>"
					}
				},
				"folder": {
					"name": "TripFaresDataPipeline"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/tripsDataSource')]",
				"[concat(variables('workspaceId'), '/datasets/tripDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSource')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSink')]",
				"[concat(variables('workspaceId'), '/dataflows/tripFaresDataTransformations')]",
				"[concat(variables('workspaceId'), '/datasets/azureSynapseAnalyticsSchema')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSynapseAnalyticsTripsData')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSynapseAnalyticsFaresData')]",
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsFaresData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"table": "FaresData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTripsData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"table": "TripsData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ParquetDate')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": "kvftr23m5xeeoklumapoc",
						"datalakeAccountName": "ftr23m5xeeoklumapoc"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "taxidata/Date",
						"fileSystem": "dlsftrpocfs1"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ParquetGeography')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": "kvftr23m5xeeoklumapoc",
						"datalakeAccountName": "ftr23m5xeeoklumapoc"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "taxidata/Geography",
						"fileSystem": "dlsftrpocfs1"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ParquetHAckneyLicense')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": "kvftr23m5xeeoklumapoc",
						"datalakeAccountName": "ftr23m5xeeoklumapoc"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "taxidata/HackeyLicense",
						"fileSystem": "dlsftrpocfs1"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ParquetMEdallion')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": "kvftr23m5xeeoklumapoc",
						"datalakeAccountName": "ftr23m5xeeoklumapoc"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "taxidata/Medallion",
						"fileSystem": "dlsftrpocfs1"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ParquetTime')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": "kvftr23m5xeeoklumapoc",
						"datalakeAccountName": "ftr23m5xeeoklumapoc"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "taxidata/Time",
						"fileSystem": "dlsftrpocfs1"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ParquetTrip')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": "kvftr23m5xeeoklumapoc",
						"datalakeAccountName": "ftr23m5xeeoklumapoc"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "taxidata/Trip",
						"fileSystem": "dlsftrpocfs1"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ParquetTrip_ByDay')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "OpdAdls",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": "kvftr23m5xeeoklumapoc"
					}
				},
				"parameters": {
					"DateId": {
						"type": "string",
						"defaultValue": "20130101"
					},
					"keyVaultName": {
						"type": "string",
						"defaultValue": "kvftr23m5xeeoklumapoc"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@concat('TripData_',dataset().DateId,'.parquet')",
							"type": "Expression"
						},
						"folderPath": {
							"value": "@concat('taxidata/Trip/',dataset().DateId)",
							"type": "Expression"
						},
						"fileSystem": "data"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/OpdAdls')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DS_ParquetWeather')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": "kvftr23m5xeeoklumapoc",
						"datalakeAccountName": "ftr23m5xeeoklumapoc"
					}
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "taxidata/Weather",
						"fileSystem": "dlsftrpocfs1"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQLPoolDate')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "HackneyLicenseID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "HackneyLicenseBKey",
						"type": "varchar"
					},
					{
						"name": "HackneyLicenseCode",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Date"
				},
				"sqlPool": {
					"referenceName": "ftr23m5xeeoklumapocws1p1",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/ftr23m5xeeoklumapocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQLPoolGeography')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "HackneyLicenseID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "HackneyLicenseBKey",
						"type": "varchar"
					},
					{
						"name": "HackneyLicenseCode",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Geography"
				},
				"sqlPool": {
					"referenceName": "ftr23m5xeeoklumapocws1p1",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/ftr23m5xeeoklumapocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQLPoolHackneyLicense')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "HackneyLicenseID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "HackneyLicenseBKey",
						"type": "varchar"
					},
					{
						"name": "HackneyLicenseCode",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "HackneyLicense"
				},
				"sqlPool": {
					"referenceName": "ftr23m5xeeoklumapocws1p1",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/ftr23m5xeeoklumapocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQLPoolMedallion')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "HackneyLicenseID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "HackneyLicenseBKey",
						"type": "varchar"
					},
					{
						"name": "HackneyLicenseCode",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Medallion"
				},
				"sqlPool": {
					"referenceName": "ftr23m5xeeoklumapocws1p1",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/ftr23m5xeeoklumapocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQLPoolTime')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "HackneyLicenseID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "HackneyLicenseBKey",
						"type": "varchar"
					},
					{
						"name": "HackneyLicenseCode",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Time"
				},
				"sqlPool": {
					"referenceName": "ftr23m5xeeoklumapocws1p1",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/ftr23m5xeeoklumapocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQLPoolTrip')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "HackneyLicenseID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "HackneyLicenseBKey",
						"type": "varchar"
					},
					{
						"name": "HackneyLicenseCode",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Trip"
				},
				"sqlPool": {
					"referenceName": "ftr23m5xeeoklumapocws1p1",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/ftr23m5xeeoklumapocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_SQLPoolWeather')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "HackneyLicenseID",
						"type": "int",
						"precision": 10
					},
					{
						"name": "HackneyLicenseBKey",
						"type": "varchar"
					},
					{
						"name": "HackneyLicenseCode",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "Weather"
				},
				"sqlPool": {
					"referenceName": "ftr23m5xeeoklumapocws1p1",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/ftr23m5xeeoklumapocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azureSynapseAnalyticsSchema')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azureSynapseAnalyticsTable')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SchemaName": {
						"type": "string"
					},
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().SchemaName",
						"type": "Expression"
					},
					"table": "AggregateTaxiData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/faresDataSink')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"datalakeAccountName": {
							"value": "@dataset().datalakeAccountName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"keyVaultName": {
						"type": "string",
						"defaultValue": "kvmsft"
					},
					"datalakeAccountName": {
						"type": "string",
						"defaultValue": "adlsmsft"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "fares-data.csv",
						"fileSystem": "public"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "medallion",
						"type": "String"
					},
					{
						"name": "hack_license",
						"type": "String"
					},
					{
						"name": "vendor_id",
						"type": "String"
					},
					{
						"name": "pickup_datetime",
						"type": "String"
					},
					{
						"name": "payment_type",
						"type": "String"
					},
					{
						"name": "fare_amount",
						"type": "String"
					},
					{
						"name": "surcharge",
						"type": "String"
					},
					{
						"name": "mta_tax",
						"type": "String"
					},
					{
						"name": "tip_amount",
						"type": "String"
					},
					{
						"name": "tolls_amount",
						"type": "String"
					},
					{
						"name": "total_amount",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/faresDataSource')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServerTripFareDataLinkedService",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServerTripFareDataLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripDataSink')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"datalakeAccountName": {
							"value": "@dataset().datalakeAccountName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"datalakeAccountName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "trip-data.csv",
						"fileSystem": "public"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "medallion",
						"type": "String"
					},
					{
						"name": "hack_license",
						"type": "String"
					},
					{
						"name": "vendor_id",
						"type": "String"
					},
					{
						"name": "rate_code",
						"type": "String"
					},
					{
						"name": "store_and_fwd_flag",
						"type": "String"
					},
					{
						"name": "pickup_datetime",
						"type": "String"
					},
					{
						"name": "dropoff_datetime",
						"type": "String"
					},
					{
						"name": "passenger_count",
						"type": "String"
					},
					{
						"name": "trip_time_in_secs",
						"type": "String"
					},
					{
						"name": "trip_distance",
						"type": "String"
					},
					{
						"name": "pickup_longitude",
						"type": "String"
					},
					{
						"name": "pickup_latitude",
						"type": "String"
					},
					{
						"name": "dropoff_longitude",
						"type": "String"
					},
					{
						"name": "dropoff_latitude",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripsDataSource')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServerTripDataLinkedService",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServerTripDataLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ADLS_Spark')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ADLS_Spark_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('ADLS_Spark_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVaultSpark')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVaultSpark_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServerTripDataLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServerTripDataLinkedService_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServerTripFareDataLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServerTripFareDataLinkedService_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_adls_sink')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('LS_adls_sink_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OpdAdls')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('OpdAdls_properties_typeProperties_url')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyVaultLinkedservice",
							"type": "LinkedServiceReference",
							"parameters": {
								"keyVaultName": "@linkedService().keyVaultName"
							}
						},
						"secretName": "OpdAdlsAccessKey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyVaultLinkedservice')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspaceTripsFares')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "",
					"tenantID": "72f988bf-86f1-41af-91ab-2d7cd011db47"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresDataLakeStorageLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					},
					"datalakeAccountName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('TripFaresDataLakeStorageLinkedService_properties_typeProperties_url')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyVaultLinkedservice",
							"type": "LinkedServiceReference",
							"parameters": {
								"keyVaultName": {
									"value": "@linkedService().keyVaultName",
									"type": "Expression"
								}
							}
						},
						"secretName": "adlsAccessKey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyVaultLinkedservice')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresSynapseAnalyticsLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('TripFaresSynapseAnalyticsLinkedService_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyVaultLinkedservice",
							"type": "LinkedServiceReference",
							"parameters": {
								"keyVaultName": {
									"value": "@linkedService().keyVaultName",
									"type": "Expression"
								}
							}
						},
						"secretName": "synapseSqlLoginPassword"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyVaultLinkedservice')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/keyVaultLinkedservice')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('keyVaultLinkedservice_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyc_tlc_yellow')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('nyc_tlc_yellow_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PauseSQLPool')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "PL-AutoPause-Resume-SQLPool",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2021-12-01T15:04:00",
						"timeZone": "Central Standard Time (Mexico)",
						"schedule": {
							"minutes": [
								5
							],
							"hours": [
								18
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/PL-AutoPause-Resume-SQLPool')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0,
							"cleanup": true
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripFaresDataTransformations')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TripFaresDataFlow"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "tripDataSink",
								"type": "DatasetReference"
							},
							"name": "TripDataCSV"
						},
						{
							"dataset": {
								"referenceName": "faresDataSink",
								"type": "DatasetReference"
							},
							"name": "FaresDataCSV"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureSynapseAnalyticsTable",
								"type": "DatasetReference"
							},
							"name": "SynapseAnalyticsSink"
						}
					],
					"transformations": [
						{
							"name": "AggregateByPaymentType"
						},
						{
							"name": "InnerJoinWithTripFares"
						}
					],
					"script": "source(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\trate_code as string,\n\t\tstore_and_fwd_flag as string,\n\t\tpickup_datetime as string,\n\t\tdropoff_datetime as string,\n\t\tpassenger_count as string,\n\t\ttrip_time_in_secs as string,\n\t\ttrip_distance as string,\n\t\tpickup_longitude as string,\n\t\tpickup_latitude as string,\n\t\tdropoff_longitude as string,\n\t\tdropoff_latitude as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tinferDriftedColumnTypes: true,\n\tignoreNoFilesFound: false) ~> TripDataCSV\nsource(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\tpickup_datetime as string,\n\t\tpayment_type as string,\n\t\tfare_amount as string,\n\t\tsurcharge as string,\n\t\tmta_tax as string,\n\t\ttip_amount as string,\n\t\ttolls_amount as string,\n\t\ttotal_amount as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tinferDriftedColumnTypes: true,\n\tignoreNoFilesFound: false) ~> FaresDataCSV\nInnerJoinWithTripFares aggregate(groupBy(payment_type),\n\taverage_fare = avg(toInteger(total_amount)),\n\t\ttotal_trip_distance = sum(toInteger(trip_distance))) ~> AggregateByPaymentType\nTripDataCSV, FaresDataCSV join(TripDataCSV@medallion == FaresDataCSV@medallion\n\t&& TripDataCSV@hack_license == FaresDataCSV@hack_license\n\t&& TripDataCSV@vendor_id == FaresDataCSV@vendor_id\n\t&& TripDataCSV@pickup_datetime == FaresDataCSV@pickup_datetime,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~> InnerJoinWithTripFares\nAggregateByPaymentType sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\trecreate:true,\n\tformat: 'table',\n\tstaged: false,\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true,\n\terrorHandlingOption: 'stopOnFirstError') ~> SynapseAnalyticsSink"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/tripDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/azureSynapseAnalyticsTable')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/4eeedd72-d937-4243-86d1-c3982a84d924/resourceGroups/nashahzsfin/providers/Microsoft.Synapse/workspaces/mfstspdjvzuh3xeu2pocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://mfstspdjvzuh3xeu2pocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 8,
						"memory": 56
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Predict NYC Taxi Tips using Spark ML and Azure Open Datasets\n",
							"\n",
							"The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
							"The goal is to predict for a given trip whether there will be a tip or not.\n",
							"\n",
							" https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-machine-learning-mllib-notebook\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"\n",
							"from pyspark.sql.functions import unix_timestamp\n",
							"\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"from pyspark.ml import Pipeline\n",
							"from pyspark.ml import PipelineModel\n",
							"from pyspark.ml.feature import RFormula\n",
							"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
							"from pyspark.ml.classification import LogisticRegression\n",
							"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
							"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Ingest Data \n",
							"\n",
							"Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Import NYC yellow cab data from Azure Open Datasets\n",
							"from azureml.opendatasets import NycTlcYellow\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"\n",
							"end_date = parser.parse('2018-05-08 00:00:00')\n",
							"start_date = parser.parse('2018-05-01 00:00:00')\n",
							"\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"#To make development easier, faster and less expensive downsample for now\n",
							"sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploratory Data Analysis\n",
							"\n",
							"Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#The charting package needs a Pandas dataframe or numpy array do the conversion\n",
							"sampled_taxi_pd_df = sampled_taxi_df.toPandas()\n",
							"\n",
							"# Look at tips by amount count histogram\n",
							"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
							"ax1.set_title('Tip amount distribution')\n",
							"ax1.set_xlabel('Tip Amount ($)')\n",
							"ax1.set_ylabel('Counts')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# How many passengers tip'd by various amounts\n",
							"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\n",
							"ax2.set_title('Tip amount by Passenger count')\n",
							"ax2.set_xlabel('Passenger count') \n",
							"ax2.set_ylabel('Tip Amount ($)')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# Look at the relationship between fare and tip amounts\n",
							"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\n",
							"ax.set_title('Tip amount by Fare amount')\n",
							"ax.set_xlabel('Fare Amount ($)')\n",
							"ax.set_ylabel('Tip Amount ($)')\n",
							"plt.axis([-2, 80, -2, 20])\n",
							"plt.suptitle('')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization\n",
							"\n",
							"It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
							"\n",
							"Finally there is a need to create some new (derived) variables that will work better with the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
							"                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
							"                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
							"                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
							"                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
							"                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
							"                                )\\\n",
							"                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
							"                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
							"                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
							"                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
							"                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
							"                                & (sampled_taxi_df.rateCodeId <= 5)\n",
							"                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
							"                                )"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization Part 2\n",
							"\n",
							"Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
							"\n",
							"Also create some more features based on new columns from the first round.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
							"                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
							"                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
							"                                                .otherwise(0).alias('trafficTimeBins')\n",
							"                                              )\\\n",
							"                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Encoding\n",
							"\n",
							"Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
							"\n",
							"The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
							"\n",
							"This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
							"sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
							"en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
							"sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
							"en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
							"\n",
							"# Create a new dataframe that has had the encodings applied\n",
							"encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Generation of Testing and Training Data Sets\n",
							"Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Decide on the split between training and testing data from the dataframe \n",
							"trainingFraction = 0.7\n",
							"testingFraction = (1-trainingFraction)\n",
							"seed = 1234\n",
							"\n",
							"# Split the dataframe into test and training dataframes\n",
							"train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train the Model\n",
							"\n",
							"Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Create a new LR object for the model\n",
							"logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
							"\n",
							"## The formula for the model\n",
							"classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
							"\n",
							"## Undertake training and create an LR model\n",
							"lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
							"\n",
							"## Saving the model is optional but its another for of inter session cache\n",
							"datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
							"fileName = \"lrModel_\" + datestamp;\n",
							"logRegDirfilename = fileName;\n",
							"lrModel.save(logRegDirfilename)\n",
							"\n",
							"## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
							"predictions = lrModel.transform(test_data_df)\n",
							"predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
							"metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
							"print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Evaluate and Visualize\n",
							"\n",
							"Plot the actual curve to develop a better understanding of the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
							"modelSummary = lrModel.stages[-1].summary\n",
							"\n",
							"plt.plot([0, 1], [0, 1], 'r--')\n",
							"plt.plot(modelSummary.roc.select('FPR').collect(),\n",
							"         modelSummary.roc.select('TPR').collect())\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MDMF_RowCount')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "felitztapia",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fb793142-95f3-4f7d-aacf-3155f3083213"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
						"name": "felitztapia",
						"type": "Spark",
						"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**IMPORTANT**\r\n",
							"Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SinkGlobalParameters =\"\"\r\n",
							"DataValidationParameters=\"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print('Obtain the parameters sent by Azure Data Factory, ***NOTE: Change above the value of \"testing=False\" for production environment.')\r\n",
							"#dbutils.widgets.text(\"DataValidationParameters\", \"\", \"\")\r\n",
							"#dv_params = dbutils.widgets.get(\"DataValidationParameters\") if testing==False else DataValidationParameters\r\n",
							"dv_params = DataValidationParameters\r\n",
							"\r\n",
							"#dbutils.widgets.text(\"SinkGlobalParameters\", \"\", \"\")\r\n",
							"#sink_params = dbutils.widgets.get(\"SinkGlobalParameters\") if testing==False else SinkGlobalParameters\r\n",
							"sink_params = SinkGlobalParameters"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"print(' Pass parameter to ADF')\r\n",
							"#dbutils.notebook.exit(json_output)\r\n",
							"mssparkutils.notebook.exit(\"hello world\") "
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MDMF_RowCount_Copy1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "felitztapia",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5565f4aa-ea7c-44ee-888a-38e71cc9d03e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
						"name": "felitztapia",
						"type": "Spark",
						"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**IMPORTANT**\r\n",
							"Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SinkGlobalParameters =\"\"\r\n",
							"DataValidationParameters=\"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print('Obtain the parameters sent by Azure Data Factory, ***NOTE: Change above the value of \"testing=False\" for production environment.')\r\n",
							"#dbutils.widgets.text(\"DataValidationParameters\", \"\", \"\")\r\n",
							"#dv_params = dbutils.widgets.get(\"DataValidationParameters\") if testing==False else DataValidationParameters\r\n",
							"dv_params = DataValidationParameters\r\n",
							"\r\n",
							"#dbutils.widgets.text(\"SinkGlobalParameters\", \"\", \"\")\r\n",
							"#sink_params = dbutils.widgets.get(\"SinkGlobalParameters\") if testing==False else SinkGlobalParameters\r\n",
							"sink_params = SinkGlobalParameters"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert string (json) parameters to dictionaries.\r\n",
							"import json\r\n",
							"\r\n",
							"dv_params_dict = dv_params #json.loads(dv_params)\r\n",
							"sink_params_dict = sink_params # json.loads(sink_params)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set(\"spark.storage.synapse.linkedServiceName\", \"ADLS_Spark\")\r\n",
							"spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\r\n",
							"spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"\r\n",
							"df = spark.read.parquet(\"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path))\r\n",
							"\r\n",
							"df.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"#replace // to get the correct path \r\n",
							"source_path = '/tripData/TripData_20130101.parquet'\r\n",
							"adls_source_name = 'aicscpgdemo02.dfs.core.windows.net'\r\n",
							"\r\n",
							"source_container_name = 'synapsedata'"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"print(' Pass parameter to ADF')\r\n",
							"#dbutils.notebook.exit(json_output)\r\n",
							"mssparkutils.notebook.exit(\"hello world\") "
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NB_01_ColumnToUpper')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "MDMF_Transformation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d1720e1c-efe2-4c9f-b3d9-41534de82a35"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Metadata-Driven Ingestion Framework \r\n",
							"#### Data Transformation: UpperCase\r\n",
							"Connect to sink instance and count the Null values from a specific column of the copied file. Validate the result and send it to Azure Data Factory."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### IMPORTANT!\r\n",
							"#### Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SinkGlobalParameters = \"{\\\"kv_scope_name\\\":\\\"KeyVaultSecrets\\\",\\\"kv_workspace_id\\\":\\\"MDIF-la-workspace-Id\\\",\\\"kv_workspace_pk\\\":\\\"MDIF-la-workspace-pk\\\",\\\"raw_storage_name\\\":\\\"\\\",\\\"raw_storage_secret_name\\\":\\\"\\\",\\\"sink_container_name\\\":\\\"sink\\\",\\\"schema_container_name\\\":\\\"schemas\\\",\\\"output_container_name\\\":\\\"datatransformation\\\",\\\"adls_storage_name\\\":\\\"adlsmetadatadriven2\\\",\\\"adls_storage_secret_name\\\":\\\"MDIF-ADLSmetadatadriven2-AccountKey\\\",\\\"sink_type\\\":\\\"ADLS\\\"}\"\r\n",
							"\r\n",
							"DataTransformationParameters = \"{\\\"DtDataSetId\\\":9,\\\"DtConfigId\\\":10,\\\"FwkConfigId\\\":\\\"26;26;26;26;26;26\\\",\\\"DtOutputId\\\":\\\"1;1;1;1;1;1\\\",\\\"ParentDtConfigId\\\":null,\\\"TriggerId\\\":3,\\\"FunctionName\\\":\\\"UpperNameProductCat\\\",\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/DataTransformation/NB_01_ColumnToUpper\\\",\\\"DtMethod\\\":\\\"Databricks\\\",\\\"SinkPath\\\":\\\"Converted/SQL/ingfrmdb/SalesLT/ProductCategory/2022/01/13/22/202201132234.csv;Converted/SQL/ingfrmdb/SalesLT/ProductCategory/2022/01/13/22/202201132234.csv;Converted/SQL/ingfrmdb/SalesLT/ProductCategory/2022/01/13/22/202201132234.csv;Converted/SQL/ingfrmdb/SalesLT/ProductCategory/2022/01/13/22/202201132234.csv;Converted/SQL/ingfrmdb/SalesLT/ProductCategory/2022/01/13/22/202201132234.csv;Converted/SQL/ingfrmdb/SalesLT/ProductCategory/2022/01/13/22/202201132234.csv\\\",\\\"OutputPath\\\":\\\"LoadedDate/LoadedDate/2022/01/05/23/LoadedDate_202201052312.csv;LoadedDate/LoadedDate/2022/01/05/23/LoadedDate_202201052312.csv;LoadedDate/LoadedDate/2022/01/05/23/LoadedDate_202201052312.csv;LoadedDate/LoadedDate/2022/01/05/23/LoadedDate_202201052312.csv;LoadedDate/LoadedDate/2022/01/05/23/LoadedDate_202201052312.csv;LoadedDate/LoadedDate/2022/01/05/23/LoadedDate_202201052312.csv\\\",\\\"FlagUpdateOutputDate\\\":\\\"0;0;0;0;0;0\\\",\\\"DtLogId\\\":\\\"39\\\",\\\"EntRunId\\\":\\\"818bddf3-c270-443d-8aca-ab4d142b75cd\\\",\\\"DtRunId\\\":\\\"93e45e67-c414-4c28-bc52-4698091ef3de\\\",\\\"DtStatus\\\":\\\"Failed\\\",\\\"OutputUpdateDate\\\":\\\"2022-01-13T22:34:17.217\\\",\\\"TransformationPathGranularity\\\":\\\"HH\\\",\\\"InputParameter\\\":\\\"{\\\\\\\"Column Name\\\\\\\":\\\\\\\"Name\\\\\\\"}\\\"}\"\r\n",
							"\r\n",
							"OutputParameters = \"[{\\\"DtDataSetId\\\":9,\\\"DtConfigId\\\":10,\\\"DtOutputId\\\":\\\"10\\\",\\\"OutputNumber\\\":\\\"1\\\",\\\"FileTransformationName\\\":\\\"UpperColName\\\",\\\"TransformationPathGranularity\\\":\\\"HH\\\",\\\"OutputUpdateDate\\\":\\\"Jan 14 2022  3:14PM\\\",\\\"DeltaTableName\\\":\\\"UpperNameSQL\\\"}]\"\r\n",
							"\r\n",
							"TriggerTime = \"2022-01-13 22:34:17\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import required libraries\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"import pandas \r\n",
							"import datetime\r\n",
							"import time"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"DataTransformationParameters=\"\"\r\n",
							"SinkGlobalParameters=\"\"\r\n",
							"output_params=\"\"\r\n",
							"TriggerTime_param=\"\"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#convert to dict \r\n",
							"dt_params_dict = json.loads(DataTransformationParameters)\r\n",
							"sink_params_dict = json.loads(SinkGlobalParameters)\r\n",
							"output_params_dict= json.loads(output_params)[0]\r\n",
							"\r\n",
							"input_parameter_dict = json.loads(dt_params_dict['InputParameter'])"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Get the necessary variables\r\n",
							"function_name = str(dt_params_dict[\"FunctionName\"]).strip()                           # Validation Function name\r\n",
							"#dt_method = str(dt_params_dict[\"DtMethod\"]).strip()                                   # Data validation method name\r\n",
							"\r\n",
							"\r\n",
							"source_path = dt_params_dict['SinkPath'].split(';')[0]                       #.split('.')[0]   #to avoid having wrong path when path repeats\r\n",
							"kv_scope_name = sink_params_dict[\"kv_scope_name\"]                                     # Name of the Azure Key Vault-backed scope\r\n",
							"kv_workspace_id = sink_params_dict[\"kv_workspace_id\"].strip()                         # Name of the secret for the log analytics workspace id\r\n",
							"kv_workspace_pk = sink_params_dict[\"kv_workspace_pk\"].strip()                         # Name of the secret for the log analytics primary key\r\n",
							"adls_storage_account_name = sink_params_dict[\"adls_storage_name\"].strip()    # Name of the Azure Blob Storage Account \r\n",
							"adls_blob_secret_name = sink_params_dict[\"adls_storage_secret_name\"].strip()            # Name of the container in the Azure Blob Storage \r\n",
							"\r\n",
							"#replace // to get the correct path \r\n",
							"source_path = source_path.replace('//', '/')\r\n",
							"adls_source_name = adls_storage_account_name + '.dfs.core.windows.net/'\r\n",
							"source_container_name = 'sink'\r\n",
							"\r\n",
							"TransformationPathGranularity = output_params_dict[\"TransformationPathGranularity\"]\r\n",
							"\r\n",
							"#storage_account_name = adls_storage_account_name              # Name of the Azure Blob Storage Account we store it in the same as adls"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"src_object_child =dt_params_dict[\"SinkPath\"].split(\"/\")[-1]\r\n",
							"#file_extension = dt_params_dict[\"SinkPath\"].split(\".\")[-1]\r\n",
							"column_name = input_parameter_dict[\"Column Name\"]\r\n",
							"#function_name = dt_params_dict[\"FunctionName\"]\r\n",
							"DtOutputId = output_params_dict[\"DtOutputId\"].split(\";\")\r\n",
							"FileTransformationName = output_params_dict[\"FileTransformationName\"].split(\";\")\r\n",
							"output_container_name = sink_params_dict[\"output_container_name\"]\r\n",
							"TriggerTime = datetime.datetime.strptime(TriggerTime_param,'%Y-%m-%d %H:%M:%S')\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#read parquet file\r\n",
							"#connet to the Sink\r\n",
							" # Function to Access Azure Blob storage using the DataFrame API reads json, csv, parquet and xml file and writes it into parquet \r\n",
							"def read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path):\r\n",
							"  \r\n",
							"  try:  \r\n",
							"    print(\"Start process\")\r\n",
							"    #Set up an account access keySet up an account access key\r\n",
							"    spark.conf.set(\"fs.azure.account.key.{}.dfs.core.windows.net\".format(adls_storage_account_name),\"{}\".format(dbutils.secrets.get(scope = \"{}\".format(kv_scope_name), key= \"{}\".format(adls_blob_secret_name))))\r\n",
							"    \"\"\"Read data Output Files and create delta tables \"\"\"\r\n",
							"    path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"\r\n",
							"    print(\"******************************** {}\".format(f'{path}*.parquet'))\r\n",
							"    try:\r\n",
							"      df = spark.read.parquet(f'{path}*.snappy.parquet')\r\n",
							"    except:\r\n",
							"      df = spark.read.parquet(f'{path}*.parquet')\r\n",
							"\r\n",
							"    \r\n",
							"    return df    \r\n",
							"  \r\n",
							"  except Exception as ex:\r\n",
							"    raise Exception(f'Error: {ex}')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sink_file=read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Remove cache files (start with \"_\")\r\n",
							"def Remove_Cache_Files_From_Sink_Transformation(files,kv_scope_name, kv_workspace_id, kv_workspace_pk):\r\n",
							"  try:\r\n",
							"    cache_files = [x for x in files if x.name.startswith(\"_\")]\r\n",
							"    for file in cache_files:\r\n",
							"      dbutils.fs.rm(file.path)\r\n",
							"      print('> Cache file removed:', file.path)\r\n",
							"  except Exception as error:\r\n",
							"    print('\\n> ***ERROR removing _cache sink files:', error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Remove cache files','FunctionName':'NB_01_ColumnToUpper'}\r\n",
							"    post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Create output path and file name\r\n",
							"def Create_Output_Path_And_File_Name(TransformationPathGranularity,output_path,FileTransformationName,TriggerTime,kv_scope_name, kv_workspace_id, kv_workspace_pk,DtOutputId,function_name,output_file_name):\r\n",
							"  \r\n",
							"  try:\r\n",
							"    i = 0\r\n",
							"    for item in DtOutputId:\r\n",
							"      if TransformationPathGranularity == \"YY\":\r\n",
							"        output_path.append(function_name + \"/\" + FileTransformationName[i] + \"/\" + str(TriggerTime.year))\r\n",
							"        output_file_name.append(FileTransformationName[i] + \"_\" + str(TriggerTime.year) + str(TriggerTime.strftime(\"%m\")) + str(TriggerTime.strftime(\"%d\")) + str(TriggerTime.strftime(\"%H\")) + str(TriggerTime.strftime(\"%M\")))\r\n",
							"      elif TransformationPathGranularity == \"MM\":\r\n",
							"        output_path.append(function_name + \"/\" + FileTransformationName[i] + \"/\" + str(TriggerTime.year) + \"/\" + str(TriggerTime.strftime(\"%m\")))\r\n",
							"        output_file_name.append(FileTransformationName[i] + \"_\" + str(TriggerTime.year) + str(TriggerTime.strftime(\"%m\")) + str(TriggerTime.strftime(\"%d\")) + str(TriggerTime.strftime(\"%H\")) + str(TriggerTime.strftime(\"%M\")))\r\n",
							"      elif TransformationPathGranularity == \"DD\":\r\n",
							"        output_path.append(function_name + \"/\" + FileTransformationName[i] + \"/\" + str(TriggerTime.year) + \"/\" + str(TriggerTime.strftime(\"%m\")) + \"/\" + str(TriggerTime.strftime(\"%d\")))\r\n",
							"        output_file_name.append(FileTransformationName[i] + \"_\" + str(TriggerTime.year) + str(TriggerTime.strftime(\"%m\")) + str(TriggerTime.strftime(\"%d\")) + str(TriggerTime.strftime(\"%H\")) + str(TriggerTime.strftime(\"%M\")))\r\n",
							"      elif TransformationPathGranularity == \"HH\":\r\n",
							"            output_path.append(function_name + \"/\" + FileTransformationName[i] + \"/\" + str(TriggerTime.year) + \"/\" + str(TriggerTime.strftime(\"%m\")) + \"/\" + str(TriggerTime.strftime(\"%d\")) + \"/\" + str(TriggerTime.strftime(\"%H\")))\r\n",
							"    output_file_name.append(FileTransformationName[i] + \"_\" + str(TriggerTime.year) + str(TriggerTime.strftime(\"%m\")) + str(TriggerTime.strftime(\"%d\")) + str(TriggerTime.strftime(\"%H\")) + str(TriggerTime.strftime(\"%M\")))\r\n",
							"    i+=1\r\n",
							"  except Exception as error:\r\n",
							"    print('\\n> ***ERROR in Create_Output_Path_And_File_Name:', error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Create output path and file name','FunctionName':'NB_01_ColumnToUpper'}\r\n",
							"    post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Transform_Column_To_Upper(sink_file,column_name,output_container_name,adls_storage_account_name,output_path,src_object_child,\r\n",
							"                              kv_scope_name, kv_workspace_id, kv_workspace_pk):\r\n",
							"    \r\n",
							"  sink_file_pandas = sink_file.toPandas()\r\n",
							"  columns = list(sink_file_pandas.columns)\r\n",
							"  file_name = src_object_child.split(\".\")[0]\r\n",
							"  delta_status = False\r\n",
							"  \r\n",
							"  if column_name in columns:\r\n",
							"    print('\\n> Uppercase characters...')\r\n",
							"    sink_file_pandas[column_name] = sink_file_pandas[column_name].str.upper()\r\n",
							"    \r\n",
							"  else:\r\n",
							"    message = f\"No column '{column_name}' in Table. Columns were found: {columns}\"\r\n",
							"    return delta_status, message\r\n",
							"    \r\n",
							"  \r\n",
							"  \r\n",
							"  try:\r\n",
							"    output_container_path = \"abfss://{}@{}.dfs.core.windows.net\".format(output_container_name, adls_storage_account_name)\r\n",
							"    output_blob_folder = \"%s/%s\" % (output_container_path,output_path[0])\r\n",
							"    print('\\n> Saving upper-part file result in:', output_blob_folder)\r\n",
							"    # Write the dataframe as a single part-file to storage, Spark uses Hadoop File Format, which requires data to be partitioned - that's why you have part- files. You can easily change filename after processing.    \r\n",
							"    output_file_dataframe = spark.createDataFrame(sink_file_pandas)\r\n",
							"    print(\"...Saving file: {}\".format(output_blob_folder))\r\n",
							"    #output_file_dataframe.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(output_blob_folder)\r\n",
							"    #output_file_dataframe.coalesce(1).write.format(\"parquet\").save(output_blob_folder)\r\n",
							"    output_file_dataframe.write.format(\"parquet\").save(output_blob_folder)\r\n",
							"\r\n",
							"    # Get the name of the wrangled-data CSV file that was just saved to Azure blob storage (it starts with 'part-')\r\n",
							"    #files = dbutils.fs.ls(output_blob_folder)\r\n",
							"    #output_file = [x for x in files if x.name.startswith(\"part-\")]\r\n",
							"    #output_file_name_id = output_file_name[0] + \".csv\"\r\n",
							"    #output_file_name_id = output_file_name[0] + \".parquet\"\r\n",
							"\r\n",
							"    #print('\\n> Renaming part-file with final name...\\n>> From: {}\\n>> To:   {}\\n'.format(output_file[0].path, \"%s/%s\" % (output_blob_folder,output_file_name_id)))\r\n",
							"    #dbutils.fs.mv(output_file[0].path, \"%s/%s\" % (output_blob_folder,output_file_name_id))\r\n",
							"    \r\n",
							"    #Delete cache files in transformation sink container _SUCCESS...\r\n",
							"    Remove_Cache_Files_From_Sink_Transformation(files,kv_scope_name, kv_workspace_id, kv_workspace_pk)\r\n",
							"    \r\n",
							"  except Exception as ex:\r\n",
							"    raise Exception(f'Error: {ex}')\r\n",
							"    \r\n",
							"  return delta_status, sink_file_pandas"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"#Create outputs (path and file name)\r\n",
							"output_path = []\r\n",
							"output_file_name = []\r\n",
							"Create_Output_Path_And_File_Name(TransformationPathGranularity,output_path,FileTransformationName,TriggerTime,kv_scope_name, kv_workspace_id, kv_workspace_pk,DtOutputId,function_name,output_file_name)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_status = False\r\n",
							"new_file = False\r\n",
							"\r\n",
							"# Obtain the count of nulls\r\n",
							"try:\r\n",
							"  delta_status, new_file = Transform_Column_To_Upper(sink_file,column_name,output_container_name,adls_storage_account_name,\r\n",
							"                                                     output_path,src_object_child,kv_scope_name, kv_workspace_id, kv_workspace_pk)\r\n",
							"  \r\n",
							"except Exception as error:\r\n",
							"  print(f\"An error has occurred: {error}\")\r\n",
							"  msg_error = {'ExecutionStatus': 'Failed','Error Message':'ERROR in UpperColumnDT','FunctionName':'NB_01_ColumnToUpper'}\r\n",
							"  post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"  \r\n",
							"  \r\n",
							"new_file"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Pass parameter to ADF\r\n",
							"mssparkutils.notebook.exit(delta_status)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NB_01_RowCount')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Metadata-Driven Ingestion Framework \nData Validation: RowCount\nConnect to source and sink instances and count the rows from the file copied. Validate the result and send it to Azure Data Factory.",
				"folder": {
					"name": "MDMF_Validation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "felitztapia",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "4cf22ae2-feef-4e52-8d8a-3976391f1a7d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
						"name": "felitztapia",
						"type": "Spark",
						"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**IMPORTANT**\r\n",
							"Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SinkGlobalParameters =\"\"\r\n",
							"DataValidationParameters=\"\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data for testing.\r\n",
							"SinkGlobalParameters =  \"{\\\"kv_scope_name\\\":\\\"ADLS_Spark\\\",\\\"kv_workspace_id\\\":\\\"MDIF-la-workspace-Id\\\",\\\"kv_workspace_pk\\\":\\\"MDIF-la-workspace-pk\\\",\\\"raw_storage_secret\\\":\\\"\\\",\\\"raw_storage_name\\\":\\\"\\\",\\\"sink_container_name\\\":\\\"sink\\\",\\\"schema_container_name\\\":\\\"schemas\\\",\\\"output_container_name\\\":\\\"datatransformation\\\",\\\"adls2_storage_account_name\\\":\\\"adlsmetadatadriven2\\\",\\\"adls2_blob_secret_name\\\":\\\"MDIF-ADLSmetadatadriven2-AccountKey\\\",\\\"sink_type\\\":\\\"ADLS\\\"}\"\r\n",
							"\r\n",
							"DataValidationParameters = \"{\\\"FwkLogId\\\":5,\\\"SrcObjectChild\\\":\\\"Product.csv\\\",\\\"DvMappingId\\\":11,\\\"SourcePath\\\":\\\"source/jsonfile/Product.json\\\",\\\"ConvertPath\\\":\\\"/Converted/ADLS/TripData/2022/01/25/16/\\\",\\\"SinkFolderPath\\\":\\\"Converted/ADLS/Files/Product/2022/01/07/15/DataValidation/\\\",\\\"FileName\\\":\\\"\\\",\\\"RowsRead\\\":null,\\\"RowsCopied\\\":null,\\\"SourceType\\\":\\\"parquet\\\",\\\"SchemaName\\\":\\\"Files\\\",\\\"SrcObject\\\":\\\"TripData_20130101.parquet\\\",\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/jsonfile\\\",\\\"IPAddress\\\":null,\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/Data Validation/NB_01_RowCount\\\",\\\"FileFormat\\\":\\\"parquet\\\",\\\"FunctionName\\\":\\\"RowCount\\\",\\\"DvMethod\\\":\\\"Databricks\\\",\\\"ConditionFlag\\\":1,\\\"EntRunId\\\":\\\"59f03a27-fa44-4c9b-92ec-598d4a808166\\\",\\\"InputParameter\\\":null}\""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print('Obtain the parameters sent by Azure Data Factory, ***NOTE: Change above the value of \"testing=False\" for production environment.')\r\n",
							"#dbutils.widgets.text(\"DataValidationParameters\", \"\", \"\")\r\n",
							"#dv_params = dbutils.widgets.get(\"DataValidationParameters\") if testing==False else DataValidationParameters\r\n",
							"dv_params = DataValidationParameters\r\n",
							"\r\n",
							"#dbutils.widgets.text(\"SinkGlobalParameters\", \"\", \"\")\r\n",
							"#sink_params = dbutils.widgets.get(\"SinkGlobalParameters\") if testing==False else SinkGlobalParameters\r\n",
							"sink_params = SinkGlobalParameters"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"# Convert string (json) parameters to dictionaries.\r\n",
							"import json\r\n",
							"\r\n",
							"dv_params_dict = json.loads(dv_params)\r\n",
							"sink_params_dict = json.loads(sink_params)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Name of the Azure Key Vault-backed scope\r\n",
							"kv_scope_name = sink_params_dict[\"kv_scope_name\"]                                     # Name of the Azure Key Vault-backed scope\r\n",
							"#kv_workspace_id = sink_params_dict[\"kv_workspace_id\"].strip()                         # Name of the secret for the log analytics workspace id\r\n",
							"#kv_workspace_pk = sink_params_dict[\"kv_workspace_pk\"].strip()                         # Name of the secret for the log analytics primary key\r\n",
							"\r\n",
							"fwklog_id = dv_params_dict[\"FwkLogId\"]\r\n",
							"function_name = dv_params_dict[\"FunctionName\"].strip()\r\n",
							"dv_method = dv_params_dict[\"DvMethod\"].strip()\r\n",
							"\r\n",
							"# Name of the Azure data lake gen 2 \r\n",
							"adls_storage_account_name = sink_params_dict[\"adls2_storage_account_name\"]    \r\n",
							"\r\n",
							"# Name of the container in the Azure data lake\r\n",
							"adls_blob_secret_name = sink_params_dict[\"adls2_blob_secret_name\"]"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source_path = dv_params_dict['ConvertPath'] \r\n",
							"print(source_path)\r\n",
							"#replace // to get the correct path \r\n",
							"source_path = source_path.replace('//', '/')\r\n",
							"adls_source_name = 'aicscpgdemo02.dfs.core.windows.net'\r\n",
							"\r\n",
							"source_container_name = 'synapsedata'\r\n",
							"\r\n",
							"converted_path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"print(converted_path)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Function to Access Azure Blob storage using the DataFrame API reads json, csv, parquet and xml file and writes it into parquet \r\n",
							"def convert_tables(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path):\r\n",
							"  \r\n",
							"  try:\r\n",
							"    \r\n",
							"    print(\"Start process\")\r\n",
							"    #Set up an account access keySet up an account access key\r\n",
							"    spark.conf.set(\"spark.storage.synapse.linkedServiceName\", kv_scope_name)\r\n",
							"    spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\r\n",
							"    spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"\r\n",
							"\r\n",
							"    row_counted = {\r\n",
							"      'row_count_converted': -1,\r\n",
							"      'row_count_landing': -1\r\n",
							"    }\r\n",
							"    \r\n",
							"    if dv_params_dict['SourceType'].lower() == 'sql': # File y Folder\r\n",
							"      print(\"SQL\")\r\n",
							"      # CONVERTED\r\n",
							"      path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"      #print(\"******************************** {}\".format(f'{path}*.snappy.parquet'))\r\n",
							"      df_converted = spark.read.parquet(f'{path}*.parquet')\r\n",
							"      row_count_converted = df_converted.count()\r\n",
							"      \r\n",
							"      \r\n",
							"      # LANDING\r\n",
							"      path_landing = dv_params_dict['ConvertPath'].replace('Converted', 'Landing')\r\n",
							"      path_landing = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, path_landing)\r\n",
							"      #print(\"******************************** {}\".format(f'{path_landing}*.snappy.parquet'))\r\n",
							"      df_landing = spark.read.parquet(f'{path}*.parquet')\r\n",
							"      row_count_landing = df_landing.count()\r\n",
							"      \r\n",
							"      row_counted['row_count_converted'] = row_count_converted\r\n",
							"      row_counted['row_count_landing'] = row_count_landing\r\n",
							"      \r\n",
							"    else: # ADLS, FTP y FS => File y Folder\r\n",
							"      \r\n",
							"      extension_file = (dv_params_dict['SrcObject'].split('.')[-1]).lower()\r\n",
							"      \r\n",
							"      #pendiente str split .\r\n",
							"      if extension_file == 'csv': #=> File y Folder\r\n",
							"        print(\"CSV FILE\")\r\n",
							"        \r\n",
							"        # CONVERTED\r\n",
							"        converted_path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"        #print(\"converted_path: \",converted_path)\r\n",
							"        df_converted = spark.read.parquet(converted_path)\r\n",
							"        row_count_converted = df_converted.count()\r\n",
							"        \r\n",
							"        \r\n",
							"        # LANDING\r\n",
							"        path_landing = dv_params_dict['ConvertPath'].replace('Converted', 'Landing')\r\n",
							"        path_landing = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, path_landing)\r\n",
							"        df_landing = spark.read.format(\"csv\").option(\"header\",\"true\").load(path_landing)\r\n",
							"        row_count_landing = df_landing.count()\r\n",
							"        \r\n",
							"        \r\n",
							"        row_counted['row_count_converted'] = row_count_converted\r\n",
							"        row_counted['row_count_landing'] = row_count_landing\r\n",
							"          \r\n",
							"      elif extension_file == 'parquet':# => File\r\n",
							"        print(\"PARQUET FILE\")\r\n",
							"        \r\n",
							"        # CONVERTED\r\n",
							"        converted_path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"        \r\n",
							"        df_converted = spark.read.parquet(converted_path)\r\n",
							"        row_count_converted = df_converted.count()\r\n",
							"        \r\n",
							"        # LANDING\r\n",
							"        path_landing = dv_params_dict['ConvertPath'].replace('Converted', 'Landing')\r\n",
							"        path_landing = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, path_landing)\r\n",
							"        \r\n",
							"        df_landing = spark.read.parquet(path_landing)\r\n",
							"        row_count_landing = df_landing.count()\r\n",
							"        \r\n",
							"        row_counted['row_count_converted'] = row_count_converted\r\n",
							"        row_counted['row_count_landing'] = row_count_landing\r\n",
							"        \r\n",
							"      elif extension_file == 'xml': #=> File y Folder pendiente hacer mount?\r\n",
							"        \r\n",
							"        print(\"XML FILE\")\r\n",
							"        # CONVERTED\r\n",
							"        converted_path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"        \r\n",
							"        df_converted = spark.read.parquet(converted_path)\r\n",
							"        row_count_converted = df_converted.count()\r\n",
							"        \r\n",
							"        # LANDING\r\n",
							"        path_landing = dv_params_dict['ConvertPath'].replace('Converted', 'Landing')\r\n",
							"        path_landing = f\"/dbfs/mnt/dataingestion/ADLS/Files/{path_landing}{dv_params_dict['SrcObject']}\"\r\n",
							"        \r\n",
							"        df = pd.read_xml(path_landing)\r\n",
							"        \r\n",
							"        #convert pandas df to spark\r\n",
							"        df_landing = spark.createDataFrame(df)\r\n",
							"        row_count_landing = df_landing.count()\r\n",
							"        \r\n",
							"        row_counted['row_count_converted'] = row_count_converted\r\n",
							"        row_counted['row_count_landing'] = row_count_landing\r\n",
							"        \r\n",
							"      \r\n",
							"      elif extension_file == 'json': # => File y Folder\r\n",
							"        print(\"JSON FILE\")\r\n",
							"        # CONVERTED\r\n",
							"        converted_path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"        #print(\"converted ==> \",converted_path)\r\n",
							"        df_converted = spark.read.parquet(converted_path)\r\n",
							"        row_count_converted = df_converted.count()\r\n",
							"        \r\n",
							"        # LANDING\r\n",
							"        path_landing = dv_params_dict['ConvertPath'].replace('Converted', 'Landing')\r\n",
							"        path_landing = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, path_landing)\r\n",
							"        #print(f\"landing : {path_landing}\")\r\n",
							"        df_landing = spark.read.option(\"multiline\",\"true\").json(path_landing)\r\n",
							"        row_count_landing = df_landing.count()\r\n",
							"        \r\n",
							"        row_counted['row_count_converted'] = row_count_converted\r\n",
							"        row_counted['row_count_landing'] = row_count_landing\r\n",
							"        #print(row_counted)\r\n",
							"        \r\n",
							"      else:\r\n",
							"        print(\"An error has occurred\")\r\n",
							"      \r\n",
							"    return row_counted\r\n",
							"      \r\n",
							"  except Exception as ex:\r\n",
							"    raise Exception(f'Error: {ex}')\r\n",
							"    \r\n",
							"    \r\n",
							"    \r\n",
							"#convert_tables(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Depending on the source, execute the function related to it\r\n",
							"sink_record_count=''\r\n",
							"try:\r\n",
							"  \r\n",
							"  sink_record_count = convert_tables(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path) \r\n",
							"  \r\n",
							"except Exception as error:\r\n",
							"  print('\\n> ***ERROR in :', error)\r\n",
							"  msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute function to validate source type','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"\r\n",
							"  \r\n",
							"print(sink_record_count)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Function to obtain values for Data Validation in sink\r\n",
							"def Validating_NB_01_RowCount(sink_record_count):\r\n",
							"  \r\n",
							"  if sink_record_count['row_count_converted'] == sink_record_count['row_count_landing']:\r\n",
							"    \r\n",
							"    validation_status = \"Succeeded\"\r\n",
							"    validation_bool = \"True\"\r\n",
							"    message = \"RowCout Validation was applied. Source and sink records match\"\r\n",
							"    record_count = sink_record_count['row_count_converted']\r\n",
							"    \r\n",
							"  else:\r\n",
							"    \r\n",
							"    validation_status = \"Failed\"\r\n",
							"    validation_bool = \"False\"\r\n",
							"    message = \"Source and sink records do not match. Source count: {}.\".format(str(source_record_count)) \r\n",
							"    record_count = 0\r\n",
							"    \r\n",
							"  output = {'ExecutionStatus': 'successfull','FwkLogId': dv_params_dict['FwkLogId'], 'Output': {'Count': str(record_count), 'Validation': { 'Status': validation_bool, 'Message': message}}}\r\n",
							"  \r\n",
							"  return output\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Count rows in sink_file and compare vs source_record_count\r\n",
							"json_output=''\r\n",
							"try:\r\n",
							"  json_output = Validating_NB_01_RowCount(sink_record_count)\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, json_output)\r\n",
							"  \r\n",
							"except:\r\n",
							"  msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to Comparing column counts','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method} #mejorar logs\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Pass parameter to ADF\r\n",
							"mssparkutils.notebook.exit(json_output)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NB_02_NullCount')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "MDMF_Validation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "felitztapia",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "311da650-36c5-4c18-9cf7-bc7a128e72da"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
						"name": "felitztapia",
						"type": "Spark",
						"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" **Metadata-Driven Ingestion Framework **\r\n",
							"### Data Validation: NullCount  ###\r\n",
							"Connect to sink instance and count the Null values from a specific column of the copied file. Validate the result and send it to Azure Data Factory.\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**IMPORTANT**\r\n",
							"Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data for testing.\r\n",
							"SinkGlobalParameters =  \"{\\\"kv_scope_name\\\":\\\"ADLS_Spark\\\",\\\"kv_workspace_id\\\":\\\"MDIF-la-workspace-Id\\\",\\\"kv_workspace_pk\\\":\\\"MDIF-la-workspace-pk\\\",\\\"raw_storage_secret\\\":\\\"\\\",\\\"raw_storage_name\\\":\\\"\\\",\\\"sink_container_name\\\":\\\"sink\\\",\\\"schema_container_name\\\":\\\"schemas\\\",\\\"output_container_name\\\":\\\"datatransformation\\\",\\\"adls2_storage_account_name\\\":\\\"adlsmetadatadriven2\\\",\\\"adls2_blob_secret_name\\\":\\\"MDIF-ADLSmetadatadriven2-AccountKey\\\",\\\"sink_type\\\":\\\"ADLS\\\"}\"\r\n",
							"\r\n",
							"DataValidationParameters = \"{\\\"FwkLogId\\\":5,\\\"SrcObjectChild\\\":\\\"Product.csv\\\",\\\"DvMappingId\\\":11,\\\"SourcePath\\\":\\\"source/jsonfile/Product.json\\\",\\\"ConvertPath\\\":\\\"/Converted/ADLS/TripData/2022/01/25/16/\\\",\\\"SinkFolderPath\\\":\\\"Converted/ADLS/Files/Product/2022/01/07/15/DataValidation/\\\",\\\"FileName\\\":\\\"\\\",\\\"RowsRead\\\":null,\\\"RowsCopied\\\":null,\\\"SourceType\\\":\\\"parquet\\\",\\\"SchemaName\\\":\\\"Files\\\",\\\"SrcObject\\\":\\\"TripData_20130101.parquet\\\",\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/jsonfile\\\",\\\"IPAddress\\\":null,\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/Data Validation/NB_01_RowCount\\\",\\\"FileFormat\\\":\\\"parquet\\\",\\\"FunctionName\\\":\\\"RowCount\\\",\\\"DvMethod\\\":\\\"Databricks\\\",\\\"ConditionFlag\\\":1,\\\"EntRunId\\\":\\\"59f03a27-fa44-4c9b-92ec-598d4a808166\\\",\\\"InputParameter\\\":null}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Import required libraries\r\n",
							"import pandas as pd\r\n",
							"import numpy\r\n",
							"#print('Running', dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get())\r\n",
							"#json_output = {}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Declaration of Variables ###"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SinkGlobalParameters =\"\"\r\n",
							"DataValidationParameters=\"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print('Obtain the parameters sent by Azure Data Factory, ***NOTE: Change above the value of \"testing=False\" for production environment.')\r\n",
							"#dbutils.widgets.text(\"DataValidationParameters\", \"\", \"\")\r\n",
							"#dv_params = dbutils.widgets.get(\"DataValidationParameters\") if testing==False else DataValidationParameters\r\n",
							"dv_params = DataValidationParameters\r\n",
							"\r\n",
							"#dbutils.widgets.text(\"SinkGlobalParameters\", \"\", \"\")\r\n",
							"#sink_params = dbutils.widgets.get(\"SinkGlobalParameters\") if testing==False else SinkGlobalParameters\r\n",
							"sink_params = SinkGlobalParameters"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert string (json) parameters to dictionaries.\r\n",
							"import json\r\n",
							"\r\n",
							"dv_params_dict = json.loads(dv_params)\r\n",
							"\r\n",
							"sink_params_dict = json.loads(sink_params)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Name of the Azure Key Vault-backed scope\r\n",
							"kv_scope_name = sink_params_dict[\"kv_scope_name\"]                                     # Name of the Azure Key Vault-backed scope\r\n",
							"kv_workspace_id =\"\" #sink_params_dict[\"kv_workspace_id\"].strip()                         # Name of the secret for the log analytics workspace id\r\n",
							"kv_workspace_pk =\"\" # sink_params_dict[\"kv_workspace_pk\"].strip()                         # Name of the secret for the log analytics primary key\r\n",
							"\r\n",
							"fwklog_id = dv_params_dict[\"FwkLogId\"]\r\n",
							"function_name = dv_params_dict[\"FunctionName\"].strip()\r\n",
							"dv_method = dv_params_dict[\"DvMethod\"].strip()\r\n",
							"\r\n",
							"# Name of the Azure data lake gen 2 \r\n",
							"adls_storage_account_name = sink_params_dict[\"adls2_storage_account_name\"]    \r\n",
							"\r\n",
							"# Name of the container in the Azure data lake\r\n",
							"adls_blob_secret_name = sink_params_dict[\"adls2_blob_secret_name\"]\r\n",
							"sink_path = dv_params_dict[\"SinkFolderPath\"].strip()                                        # Path of the sink file\r\n",
							"file_extension = 'parquet'\r\n",
							"# Format of the sink file\r\n",
							"condition = dv_params_dict[\"ConditionFlag\"]                                   # UPDATE ConditionFlag new value\r\n",
							"\r\n",
							"# Declare input parameter variable\r\n",
							"column_names = (dv_params_dict['InputParameter']['Column Name']).split(',')\r\n",
							"#column_names = [columns.lower() for columns in column_names]\r\n",
							"column_names"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source_path = dv_params_dict['ConvertPath']\r\n",
							"#replace // to get the correct path \r\n",
							"source_path = source_path.replace('//', '/')\r\n",
							"adls_source_name = adls_storage_account_name + '.dfs.core.windows.net/'\r\n",
							"\r\n",
							"source_container_name = sink_params_dict[\"sink_container_name\"]\r\n",
							"sink_container_name = sink_params_dict[\"sink_container_name\"]"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Function to Access Azure Blob storage using the DataFrame API reads json, csv, parquet and xml file and writes it into parquet \r\n",
							"def read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path):\r\n",
							"  \r\n",
							"  try:   \r\n",
							"    print(\"Start process\")\r\n",
							"     #Set up an account access keySet up an account access key\r\n",
							"    spark.conf.set(\"spark.storage.synapse.linkedServiceName\", kv_scope_name)\r\n",
							"    spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\r\n",
							"    spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"    \"\"\"Read data Output Files and create delta tables \"\"\"\r\n",
							"    path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"\r\n",
							"    print(\"******************************** {}\".format(f'{path}*.parquet'))\r\n",
							"    df = spark.read.parquet(f'{path}*.parquet')\r\n",
							"    \r\n",
							"    \r\n",
							"    return df    \r\n",
							"  \r\n",
							"  except Exception as ex:\r\n",
							"    raise Exception(f'Error: {ex}')\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"df=read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path)\r\n",
							"  "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def nullcount(file,cols): #function to count null values\r\n",
							"  nullarray=[]\r\n",
							"  cont=0  \r\n",
							"  for col in cols:\r\n",
							"    try:\r\n",
							"      cont=cont+file.filter(file[col] == \"NULL\").count()\r\n",
							"      cont=cont+file.filter(file[col] == \"\").count()  \r\n",
							"    except Exception as ex:\r\n",
							"      raise Exception(f'Error: {ex}')\r\n",
							"    nullarray.append(cont)\r\n",
							"  return nullarray"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Depending on condition, the outpout is changed: 1=Just Logs, 2=Separate Good AND Corrupted Data. \r\n",
							"def condition_output(file, column_names, condition, sink_container_name, sink_path, file_extension, adls_storage_account_name,kv_scope_name, kv_workspace_id, kv_workspace_pk,function_name,dv_method,fwklog_id): \r\n",
							"  try:\r\n",
							"    if str(condition) == '1':\r\n",
							"      print(\"\\n> Running condition 1\")\r\n",
							"    elif str(condition) == '2':\r\n",
							"      print(\"\\n> Running condition 2, file:\", file)\r\n",
							"      file = file.toPandas()\r\n",
							"      #print(\"\\n> file to pandas:\", file)\r\n",
							"      corrupt = pd.DataFrame(columns = list(file.columns))\r\n",
							"      #print('*** 1 *** ')\r\n",
							"      for i in range(len(column_names)):\r\n",
							"        count_regex = 0\r\n",
							"        x=0\r\n",
							"        for item in file[column_names[i]]:          \r\n",
							"          y=0\r\n",
							"          if item is None or item.lower() in (\"null\",\"none\",\"nan\"):            \r\n",
							"            corrupt.loc[len(corrupt.index)]=list(file.loc[x])\r\n",
							"          x= x+1\r\n",
							"      #Load files as csv but if corrupt is empty then pass\r\n",
							"      if corrupt.empty:\r\n",
							"        print('No corrupt values')\r\n",
							"        pass\r\n",
							"      else:\r\n",
							"        SinkValid = sink_path.replace('.'+file_extension, \"\")+'_NullCount_Valid'\r\n",
							"        SinkInvalid = sink_path.replace('.'+file_extension, \"\")+'_NullCount_Invalid'\r\n",
							"        corrupt = corrupt.drop_duplicates(keep=\"first\")\r\n",
							"        join = pd.concat([file, corrupt])\r\n",
							"        correct = join.drop_duplicates(keep=False)\r\n",
							"        correctSpk = spark.createDataFrame(correct)\r\n",
							"        correctSpk.write.format(\"parquet\").save(\"abfss://{}@{}.dfs.core.windows.net/{}\".format(sink_container_name, adls_storage_account_name, SinkValid))\r\n",
							"        #correctSpk.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"abfss://{}@{}.dfs.core.windows.net/{}\".format(sink_container_name, adls_storage_account_name, SinkValid))\r\n",
							"        corrupt = corrupt.fillna('')      \r\n",
							"        corruptSpk = spark.createDataFrame(corrupt) #To Fix Error here: Can not infer schema from empty dataset. https://www.perfectlyrandom.org/bites/2019/10/16/empty-spark-dataframes\r\n",
							"        print(\"writing corrupt data\")\r\n",
							"        corrup_path = \"abfss://{}@{}.dfs.core.windows.net/{}\".format(sink_container_name, adls_storage_account_name, SinkInvalid)\r\n",
							"        print(corrup_path)\r\n",
							"        corruptSpk.write.format(\"parquet\").save(corrup_path)\r\n",
							"                                       \r\n",
							"        #corruptSpk.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode(\"overwrite\").save(corrup_path)\r\n",
							"        print(\"\\n> Currated files loaded successfully.\")\r\n",
							"    else:\r\n",
							"      print(\"\\n> Not valid ConditionFlag number\")\r\n",
							"    return \"Condition {0} took place\".format(condition)\r\n",
							"  \r\n",
							"  except Exception as error:\r\n",
							"    print(\"*** ERROR in condition_output:\", error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute function split data','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"    #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"    return msg_error\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Main method for validation.\r\n",
							"def Validating_NB_02_NullCount(df,column_names,condition,sink_container_name,sink_path,file_extension,adls_storage_account_name,fwklog_id,function_name,dv_method,kv_scope_name, kv_workspace_id, kv_workspace_pk):\r\n",
							"  try:\r\n",
							"    sink_null_array=nullcount(df,column_names)\r\n",
							"    print('\\n> sink_null_array:', sink_null_array)\r\n",
							"    condition_message=condition_output(df, column_names, condition, sink_container_name, sink_path, file_extension, adls_storage_account_name,kv_scope_name, kv_workspace_id, kv_workspace_pk,function_name,dv_method,fwklog_id)\r\n",
							"    print(\"condition_message : {}\".format(condition_message))\r\n",
							"    validation_status = \"NULL\"\r\n",
							"    validation_bool = \"True\"\r\n",
							"    #message = ''\r\n",
							"\r\n",
							"    for i in range(len(sink_null_array)):\r\n",
							"      message = \"NullCount Validation was applied. The '{}' column has '{}' Null values. \".format(column_names[i], sink_null_array[i])\r\n",
							"      if sink_null_array[i] > 0:\r\n",
							"        validation_bool = \"False\"\r\n",
							"    return {'ExecutionStatus': 'successfull',\"FwkLogId\": fwklog_id, \"Output\": {\"Count\": sink_null_array, \"Validation\": { \"Status\": validation_bool, \"Message\": message}}}\r\n",
							"  except Exception as error:\r\n",
							"    print(\"*** ERROR in Validating_NB_02_NullCount:\", error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute Main method for validation','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"    #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"    return msg_error\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Build the inputs for the DataValidationLog table\r\n",
							"json_output={'ExecutionStatus': 'N/A','Error Message':'Fail to Build the output for the DataValidationLog table'}\r\n",
							"try:\r\n",
							"  json_output = Validating_NB_02_NullCount(df,column_names,condition,sink_container_name,sink_path,file_extension,adls_storage_account_name,fwklog_id,function_name,dv_method,kv_scope_name, kv_workspace_id, kv_workspace_pk)\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, json_output)\r\n",
							"except Exception as err:\r\n",
							"  print(f\"An error has occurred...{err}\")\r\n",
							"  msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to Build the inputs for the DataValidationLog table','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Pass parameter to ADF\r\n",
							"mssparkutils.notebook.exit(json_output)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NB_03_ColumnLevel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "MDMF_Validation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "felitztapia",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6171b83d-2284-48c9-9d67-c197fbd54dc9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
						"name": "felitztapia",
						"type": "Spark",
						"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### IMPORTANT!\r\n",
							"#### Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data for testing.\r\n",
							"SinkGlobalParameters =  \"{\\\"kv_scope_name\\\":\\\"ADLS_Spark\\\",\\\"kv_workspace_id\\\":\\\"MDIF-la-workspace-Id\\\",\\\"kv_workspace_pk\\\":\\\"MDIF-la-workspace-pk\\\",\\\"raw_storage_secret\\\":\\\"\\\",\\\"raw_storage_name\\\":\\\"\\\",\\\"sink_container_name\\\":\\\"sink\\\",\\\"schema_container_name\\\":\\\"schemas\\\",\\\"output_container_name\\\":\\\"datatransformation\\\",\\\"adls2_storage_account_name\\\":\\\"adlsmetadatadriven2\\\",\\\"adls2_blob_secret_name\\\":\\\"MDIF-ADLSmetadatadriven2-AccountKey\\\",\\\"sink_type\\\":\\\"ADLS\\\"}\"\r\n",
							"\r\n",
							"DataValidationParameters = \"{\\\"FwkLogId\\\":5,\\\"SrcObjectChild\\\":\\\"Product.csv\\\",\\\"DvMappingId\\\":11,\\\"SourcePath\\\":\\\"source/jsonfile/Product.json\\\",\\\"ConvertPath\\\":\\\"/Converted/ADLS/TripData/2022/01/25/16/\\\",\\\"SinkFolderPath\\\":\\\"Converted/ADLS/Files/Product/2022/01/07/15/DataValidation/\\\",\\\"FileName\\\":\\\"\\\",\\\"RowsRead\\\":null,\\\"RowsCopied\\\":null,\\\"SourceType\\\":\\\"parquet\\\",\\\"SchemaName\\\":\\\"Files\\\",\\\"SrcObject\\\":\\\"TripData_20130101.parquet\\\",\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/jsonfile\\\",\\\"IPAddress\\\":null,\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/Data Validation/NB_01_RowCount\\\",\\\"FileFormat\\\":\\\"parquet\\\",\\\"FunctionName\\\":\\\"RowCount\\\",\\\"DvMethod\\\":\\\"Databricks\\\",\\\"ConditionFlag\\\":1,\\\"EntRunId\\\":\\\"59f03a27-fa44-4c9b-92ec-598d4a808166\\\",\\\"InputParameter\\\":null}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import required libraries\r\n",
							"import numpy\r\n",
							"import pandas as pd\r\n",
							"import re\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SinkGlobalParameters =\"\"\r\n",
							"DataValidationParameters=\"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print('Obtain the parameters sent by Azure Data Factory, ***NOTE: Change above the value of \"testing=False\" for production environment.')\r\n",
							"#dbutils.widgets.text(\"DataValidationParameters\", \"\", \"\")\r\n",
							"#dv_params = dbutils.widgets.get(\"DataValidationParameters\") if testing==False else DataValidationParameters\r\n",
							"dv_params = DataValidationParameters\r\n",
							"\r\n",
							"#dbutils.widgets.text(\"SinkGlobalParameters\", \"\", \"\")\r\n",
							"#sink_params = dbutils.widgets.get(\"SinkGlobalParameters\") if testing==False else SinkGlobalParameters\r\n",
							"sink_params = SinkGlobalParameters"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert string (json) parameters to dictionaries.\r\n",
							"import json\r\n",
							"\r\n",
							"dv_params_dict = json.loads(dv_params)\r\n",
							"\r\n",
							"sink_params_dict = json.loads(sink_params)\r\n",
							"input_parameter_dict=dv_params_dict['InputParameter']"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Get the necessary variables\r\n",
							"fwklog_id = dv_params_dict[\"FwkLogId\"]  \r\n",
							"function_name = str(dv_params_dict[\"FunctionName\"]).strip()                           # Validation Function name\r\n",
							"dv_method = str(dv_params_dict[\"DvMethod\"]).strip()                                   # Data validation method name\r\n",
							"\r\n",
							"\r\n",
							"source_path = dv_params_dict['ConvertPath']\r\n",
							"#replace // to get the correct path \r\n",
							"source_path = source_path.replace('//', '/')\r\n",
							"sink_path = dv_params_dict[\"SinkFolderPath\"].strip()                                        # Path of the sink file\r\n",
							"sink_container_name = sink_params_dict[\"sink_container_name\"].strip() #its also sink\r\n",
							"source_container_name =  sink_params_dict[\"sink_container_name\"]\r\n",
							"\r\n",
							"kv_scope_name = sink_params_dict[\"kv_scope_name\"]                                     # Name of the Azure Key Vault-backed scope\r\n",
							"#kv_workspace_id = sink_params_dict[\"kv_workspace_id\"].strip()                         # Name of the secret for the log analytics workspace id\r\n",
							"#kv_workspace_pk = sink_params_dict[\"kv_workspace_pk\"].strip()                         # Name of the secret for the log analytics primary key\r\n",
							"adls_storage_account_name = sink_params_dict[\"adls_storage_name\"].strip()    # Name of the Azure Blob Storage Account \r\n",
							"adls_blob_secret_name = sink_params_dict[\"adls_storage_secret_name\"].strip()            # Name of the container in the Azure Blob Storage \r\n",
							"\r\n",
							"storage_account_name = adls_storage_account_name              # Name of the Azure Blob Storage Account we store it in the same as adls\r\n",
							"adls_source_name = adls_storage_account_name + '.dfs.core.windows.net/'\r\n",
							"\r\n",
							"condition = dv_params_dict[\"ConditionFlag\"]                                 # UPDATE ConditionFlag new value\r\n",
							"column_names = numpy.array(list(input_parameter_dict.keys())) \r\n",
							"target_expressions = numpy.array(list(input_parameter_dict.values()))                  # Array with the regular expressions"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#connet to the Sink\r\n",
							" # Function to Access Azure Blob storage using the DataFrame API reads json, csv, parquet and xml file and writes it into parquet \r\n",
							"def read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path):\r\n",
							"  \r\n",
							"  \r\n",
							"  try:\r\n",
							"    \r\n",
							"    print(\"Start process\")\r\n",
							"     #Set up an account access keySet up an account access key\r\n",
							"    spark.conf.set(\"spark.storage.synapse.linkedServiceName\", kv_scope_name)\r\n",
							"    spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\r\n",
							"    spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"    \"\"\"Read data Output Files and create delta tables \"\"\"\r\n",
							"    path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"\r\n",
							"    print(\"******************************** {}\".format(f'{path}*.parquet'))\r\n",
							"    try:\r\n",
							"      df = spark.read.parquet(f'{path}*.snappy.parquet')\r\n",
							"    except:\r\n",
							"      df = spark.read.parquet(f'{path}*.parquet')\r\n",
							"\r\n",
							"    \r\n",
							"    return df    \r\n",
							"  \r\n",
							"  except Exception as ex:\r\n",
							"    raise Exception(f'Error: {ex}')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path)\r\n",
							"\r\n",
							"#display(df)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Function to count values that do not meet the regex parameter in a specified coulmn from the loaded file\r\n",
							"def count_regex(file, column_names, target_expressions):\r\n",
							"  try:\r\n",
							"    \"\"\" Count the number of input parameter in file \"\"\"\r\n",
							"    file = file.toPandas()\r\n",
							"    regex_array=[]\r\n",
							"    for i in range(len(column_names)):\r\n",
							"      count_regex = 0\r\n",
							"      for item in file[column_names[i]]:\r\n",
							"        if item is None:\r\n",
							"          count_regex = count_regex + 1 \r\n",
							"        elif not re.match(target_expressions[i], str(item), flags=0): #regex function expects string so convert item to string\r\n",
							"          count_regex = count_regex + 1\r\n",
							"      regex_array.append(count_regex) \r\n",
							"    return regex_array\r\n",
							"  except Exception as error:\r\n",
							"    print(\"*** ERROR in count_regex:\", error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute function to count values that do not meet the regex parameter in a specified coulmn from the loaded file','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"    #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"    return msg_error"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Depending on condition, the outpout is changed: 1=Just Logs, 2=Separate Good AND Corrupted Data.\r\n",
							"def condition_output(file, column_names, condition, target_expressions, sink_container_name, storage_account_name, sink_path, adls_storage_account_name):\r\n",
							"  try:        \r\n",
							"    if str(condition) == '1':\r\n",
							"      print(\"\\n> Running condition 1\")\r\n",
							"      \r\n",
							"    elif str(condition) == '2':\r\n",
							"      file = file.toPandas()\r\n",
							"      corrupt = pd.DataFrame(columns = list(file.columns))\r\n",
							"\r\n",
							"      for i in range(len(column_names)):\r\n",
							"        count_regex = 0\r\n",
							"        x=0\r\n",
							"        for item in file[column_names[i]]:\r\n",
							"          y=0\r\n",
							"          if item is None:\r\n",
							"            corrupt.loc[len(corrupt.index)]=list(file.loc[x])\r\n",
							"          elif not re.match(target_expressions[i], item, flags=0): \r\n",
							"            corrupt.loc[len(corrupt.index)]=list(file.loc[x])\r\n",
							"          x= x+1\r\n",
							"  \r\n",
							"      corrupt = corrupt.drop_duplicates(keep=\"first\") #Corrupt df\r\n",
							"      join = pd.concat([file, corrupt]) #file - corrupt = correct\r\n",
							"      correct = join.drop_duplicates(keep=False) #Correct df\r\n",
							"      if corrupt.empty:\r\n",
							"        print('No corrupt values')\r\n",
							"        pass\r\n",
							"      else:\r\n",
							"      #SinkValid = sink_path.replace('.'+file_extension, \"\")+'_ColumnLevel_Valid'\r\n",
							"      #SinkInvalid = sink_path.replace('.'+file_extension, \"\")+'_ColumnLevel_Invalid'\r\n",
							"        SinkValid = sink_path.replace('.parquet', \"\")+'_ColumnLevel_Valid'\r\n",
							"        SinkInvalid = sink_path.replace('.parquet', \"\")+'_ColumnLevel_Invalid'\r\n",
							"\r\n",
							"        correctSpk = spark.createDataFrame(correct)\r\n",
							"        #correctSpk.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"abfss://{}@{}.dfs.core.windows.net/{}\".format(sink_container_name, adls_storage_account_name, SinkValid))\r\n",
							"        save_valid_path=\"abfss://{}@{}.dfs.core.windows.net/{}\".format(sink_container_name,adls_storage_account_name,SinkValid)\r\n",
							"        correctSpk.write.format(\"parquet\").mode(\"overwrite\").save(save_valid_path)                     \r\n",
							"        \r\n",
							"        corruptSpk = spark.createDataFrame(corrupt)\r\n",
							"        save_invalid_path=\"abfss://{}@{}.dfs.core.windows.net/{}\".format(sink_container_name,adls_storage_account_name,SinkInvalid)\r\n",
							"        corruptSpk.write.format(\"parquet\").mode(\"overwrite\").save(\"abfss://{}@{}.dfs.core.windows.net/{}\".format(save_invalid_path))\r\n",
							"                                                          \r\n",
							"        #corruptSpk.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").mode(\"overwrite\").save(\"abfss://{}@{}.dfs.core.windows.net/{}\".format(sink_container_name, adls_storage_account_name, SinkInvalid))\r\n",
							"        print(\"> Currated (Valid & Invalid) files loaded\")\r\n",
							"    else:\r\n",
							"      print(\"> Not valid ConditionFlag number\")\r\n",
							"      \r\n",
							"    return \"Condition {0} took place\".format(condition)\r\n",
							"  \r\n",
							"  except Exception as error:\r\n",
							"    print(\"*** ERROR in condition_output:\", error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute function to split data','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"    #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"    return msg_error"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Build the inputs for the DataValidationLog table\r\n",
							"def Validate_Column_Regex(sink_regex_array,column_names):\r\n",
							"  try:\r\n",
							"    validation = 0\r\n",
							"    failed_array = []\r\n",
							"    failed_columns = []\r\n",
							"    #message = \"\"\r\n",
							"\r\n",
							"    for i in range(len(sink_regex_array)):\r\n",
							"      if sink_regex_array[i] > 0:\r\n",
							"        validation = validation + 1\r\n",
							"        failed_array.append(sink_regex_array[i])\r\n",
							"        failed_columns.append(column_names[i])\r\n",
							"\r\n",
							"    if validation == 0:\r\n",
							"      validation_status = \"Succeeded\"\r\n",
							"      validation_bool = \"True\"\r\n",
							"      message = \"ColumnLevel Validation was applied. All the values follow the regular expression(s).\"\r\n",
							"    else:\r\n",
							"      validation_status = \"Failed\"\r\n",
							"      validation_bool = \"False\"\r\n",
							"      for i in range(len(failed_array)):\r\n",
							"        message = \"The column {} has {} values that not follow the regular expression parameter. \".format(failed_columns[i], failed_array[i])\r\n",
							"\r\n",
							"    output = {'ExecutionStatus': 'Successfull',\"FwkLogId\": fwklog_id, \"Output\": {\"Count\": sink_regex_array, \"Validation\": { \"Status\": validation_bool, \"Message\": message}}}\r\n",
							"    return output\r\n",
							"  \r\n",
							"  except Exception as error:\r\n",
							"    print(\"*** ERROR in Validate_Column_Regex:\", error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute Build the inputs for the DataValidationLog table','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"    #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"    return msg_error"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Main method for validation.\r\n",
							"def Validating_NB_03_ColumnLevel(sink_file,column_names,target_expressions,condition,sink_container_name,storage_account_name,sink_path,adls_storage_account_name):\r\n",
							"  try:\r\n",
							"    sink_regex_array = count_regex(sink_file, column_names, target_expressions)\r\n",
							"    print(sink_regex_array)\r\n",
							"    if sink_regex_array != {}:\r\n",
							"      condition_message = condition_output(sink_file, column_names, condition, target_expressions, sink_container_name, storage_account_name, sink_path, adls_storage_account_name)\r\n",
							"      if condition_message != \"\":\r\n",
							"        return Validate_Column_Regex(sink_regex_array,column_names)\r\n",
							"    return {}\r\n",
							"  except Exception as error:\r\n",
							"    print(\"*** ERROR:\", error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute ain method for validation','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"    #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"    return msg_error"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Checking column(s) with regex.\r\n",
							"json_output={'ExecutionStatus': 'N/A','Error Message':'Fail at Checking columns with regex.'}\r\n",
							"try:\r\n",
							"  json_output = Validating_NB_03_ColumnLevel(df,column_names,target_expressions,condition,sink_container_name,storage_account_name,sink_path,adls_storage_account_name)\r\n",
							"  post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, json_output)\r\n",
							"except:\r\n",
							"  msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail at Checking columns with regex','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Pass parameter to ADF\r\n",
							"mssparkutils.notebook.exit(json_output)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NB_04_RecordLevel')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "MDMF_Validation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "felitztapia",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3e93f2ac-aa56-4403-ba4f-3538058f0dbd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
						"name": "felitztapia",
						"type": "Spark",
						"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Metadata-Driven Ingestion Framework \r\n",
							"#### Data Validation: 04 RecordLevel\r\n",
							"Connect to sink instance and count the columns from the file copied. Validate the result and send it to Azure Data Factory."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### IMPORTANT!\r\n",
							"#### Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data for testing.\r\n",
							"SinkGlobalParameters =  \"{\\\"kv_scope_name\\\":\\\"ADLS_Spark\\\",\\\"kv_workspace_id\\\":\\\"MDIF-la-workspace-Id\\\",\\\"kv_workspace_pk\\\":\\\"MDIF-la-workspace-pk\\\",\\\"raw_storage_secret\\\":\\\"\\\",\\\"raw_storage_name\\\":\\\"\\\",\\\"sink_container_name\\\":\\\"sink\\\",\\\"schema_container_name\\\":\\\"schemas\\\",\\\"output_container_name\\\":\\\"datatransformation\\\",\\\"adls2_storage_account_name\\\":\\\"adlsmetadatadriven2\\\",\\\"adls2_blob_secret_name\\\":\\\"MDIF-ADLSmetadatadriven2-AccountKey\\\",\\\"sink_type\\\":\\\"ADLS\\\"}\"\r\n",
							"\r\n",
							"DataValidationParameters = \"{\\\"FwkLogId\\\":5,\\\"SrcObjectChild\\\":\\\"Product.csv\\\",\\\"DvMappingId\\\":11,\\\"SourcePath\\\":\\\"source/jsonfile/Product.json\\\",\\\"ConvertPath\\\":\\\"/Converted/ADLS/TripData/2022/01/25/16/\\\",\\\"SinkFolderPath\\\":\\\"Converted/ADLS/Files/Product/2022/01/07/15/DataValidation/\\\",\\\"FileName\\\":\\\"\\\",\\\"RowsRead\\\":null,\\\"RowsCopied\\\":null,\\\"SourceType\\\":\\\"parquet\\\",\\\"SchemaName\\\":\\\"Files\\\",\\\"SrcObject\\\":\\\"TripData_20130101.parquet\\\",\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/jsonfile\\\",\\\"IPAddress\\\":null,\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/Data Validation/NB_01_RowCount\\\",\\\"FileFormat\\\":\\\"parquet\\\",\\\"FunctionName\\\":\\\"RowCount\\\",\\\"DvMethod\\\":\\\"Databricks\\\",\\\"ConditionFlag\\\":1,\\\"EntRunId\\\":\\\"59f03a27-fa44-4c9b-92ec-598d4a808166\\\",\\\"InputParameter\\\":null}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SinkGlobalParameters =\"\"\r\n",
							"DataValidationParameters=\"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert string (json) parameters to dictionaries.\r\n",
							"import json\r\n",
							"\r\n",
							"dv_params_dict = json.loads(dv_params)\r\n",
							"sink_params_dict = json.loads(sink_params)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Declare and fill general variables\r\n",
							"fwklog_id = str(dv_params_dict[\"FwkLogId\"]).strip()                                   # ID of the Framework Log\r\n",
							"function_name = str(dv_params_dict[\"FunctionName\"]).strip()                           # Validation Function name\r\n",
							"dv_method = str(dv_params_dict[\"DvMethod\"]).strip()                                   # Data validation method name\r\n",
							"kv_scope_name = sink_params_dict[\"kv_scope_name\"].strip()                             # Name of the Azure Key Vault-backed scope\r\n",
							"kv_workspace_id = sink_params_dict[\"kv_workspace_id\"].strip()                         # Name of the secret for the log analytics workspace id\r\n",
							"kv_workspace_pk = sink_params_dict[\"kv_workspace_pk\"].strip()                         # Name of the secret for the log analytics primary key\r\n",
							"# Declare sink variables \r\n",
							"#Key on AKV\r\n",
							"adls_storage_account_name = sink_params_dict[\"adls_storage_name\"].strip()    # Name of the Azure Blob Storage Account \r\n",
							"adls_blob_secret_name = sink_params_dict[\"adls_storage_secret_name\"].strip()            # Name of the container in the Azure Blob Storage Account \r\n",
							"sink_path = dv_params_dict[\"ConvertPath\"].strip()                                     # Path of the sink file\r\n",
							"# Declare input parameter variable\r\n",
							"\r\n",
							"input_parameter_dict = json.loads(dv_params_dict['InputParameter'])\r\n",
							"\r\n",
							"column_names = input_parameter_dict[\"Column Name\"] if \"Column Name\" in input_parameter_dict else '' # Name(s) of column(s) to validate, there may be no parameters (for example NB_01) \r\n",
							"\r\n",
							"# Declare variables for NB_04_RecordLevel\r\n",
							"target_count = int(input_parameter_dict[\"Target Count\"])                                   # Expected number of columns from the sink file\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"source_container_name = 'sink'\r\n",
							"source_path = sink_path.replace('//', '/')\r\n",
							"#adls_source_name = 'adlsmetadatadriven2.dfs.core.windows.net/'\r\n",
							"adls_source_name = adls_storage_account_name + '.dfs.core.windows.net/'\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def count_columns(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_path):\r\n",
							"  \r\n",
							"    try:\r\n",
							"        print(\"Start process\")\r\n",
							"        #Set up an account access keySet up an account access key\r\n",
							"        spark.conf.set(\"spark.storage.synapse.linkedServiceName\", kv_scope_name)\r\n",
							"        spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\r\n",
							"        spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"        \r\n",
							"        path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"        print(\"***** {}\".format(f\"{path}*.parquet\"))\r\n",
							"        \r\n",
							"        df_converted = spark.read.parquet(f\"{path}*.parquet\")\r\n",
							"        size = len(df_converted.columns)\r\n",
							"        #print(\"=========> \",size)\r\n",
							"        \r\n",
							"        return size\r\n",
							"      \r\n",
							"    except Exception as err:\r\n",
							"        raise Exception(f\"{err}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Main method for validation.\r\n",
							"def Validating_NB_04_RecordLevel(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, target_count, source_path):\r\n",
							"  try:\r\n",
							"    sink_column_count = count_columns(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_path)\r\n",
							"    output = Compare_Count_Columns(target_count, sink_column_count)\r\n",
							"    \r\n",
							"    return output\r\n",
							"  \r\n",
							"  except Exception as error:\r\n",
							"    print(\"*** ERROR:\", error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute main method for validation','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"    #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"    \r\n",
							"    return msg_error"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Build the inputs for the DataValidationLog table\r\n",
							"def Compare_Count_Columns(target_count, sink_column_count):\r\n",
							"  \r\n",
							"  print('>Comparing count columns:\\n>> Target: {}\\n>> Sink: {}'.format(target_count, sink_column_count))\r\n",
							"  \r\n",
							"  if target_count == sink_column_count:\r\n",
							"    validation_status = \"Succeeded\"\r\n",
							"    validation_bool = \"True\"\r\n",
							"    message = \"Target Count parameter and sink column count do match.\"\r\n",
							"    \r\n",
							"  else:\r\n",
							"    \r\n",
							"    validation_status = \"Failed\"\r\n",
							"    validation_bool = \"False\"\r\n",
							"    message = \"Target Count parameter and sink column count do not match. Target count: {} but were found {}\".format(target_count, sink_column_count)\r\n",
							"  \r\n",
							"  output = {'ExecutionStatus': 'Successfull', \"FwkLogId\": fwklog_id, \"Output\": {\"Count\": sink_column_count, \"Validation\": { \"Status\": validation_bool, \"Message\": message}}}\r\n",
							"  return output"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Declaration of variables and execution of functions"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Comparing column counts.\r\n",
							"try:\r\n",
							"  json_output = Validating_NB_04_RecordLevel(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, target_count, source_path)\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, json_output)\r\n",
							"  \r\n",
							"except:\r\n",
							"  msg_error = {'ExecutionStatus': 'Failed', 'Error Message':'Fail in Record Level', 'FwkLogId': fwklog_id, 'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"  \r\n",
							"  \r\n",
							"json_output"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Pass parameter to ADF\r\n",
							"mssparkutils.notebook.exit(json_output)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NB_05_MinMaxRowCount')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "MDMF_Validation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "felitztapia",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1f860a2e-d0c0-4fcc-a7b2-0887062869b5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
						"name": "felitztapia",
						"type": "Spark",
						"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Metadata-Driven Ingestion Framework \r\n",
							"#### Data Validation: 05 Min/Max Row Count\r\n",
							"Connect to sink instance and count the records. Compare the result with the input parameters send it to Azure Data Factory."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### IMPORTANT!\r\n",
							"#### Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data for testing.\r\n",
							"SinkGlobalParameters =  \"{\\\"kv_scope_name\\\":\\\"ADLS_Spark\\\",\\\"kv_workspace_id\\\":\\\"MDIF-la-workspace-Id\\\",\\\"kv_workspace_pk\\\":\\\"MDIF-la-workspace-pk\\\",\\\"raw_storage_secret\\\":\\\"\\\",\\\"raw_storage_name\\\":\\\"\\\",\\\"sink_container_name\\\":\\\"sink\\\",\\\"schema_container_name\\\":\\\"schemas\\\",\\\"output_container_name\\\":\\\"datatransformation\\\",\\\"adls2_storage_account_name\\\":\\\"adlsmetadatadriven2\\\",\\\"adls2_blob_secret_name\\\":\\\"MDIF-ADLSmetadatadriven2-AccountKey\\\",\\\"sink_type\\\":\\\"ADLS\\\"}\"\r\n",
							"\r\n",
							"DataValidationParameters = \"{\\\"FwkLogId\\\":5,\\\"SrcObjectChild\\\":\\\"Product.csv\\\",\\\"DvMappingId\\\":11,\\\"SourcePath\\\":\\\"source/jsonfile/Product.json\\\",\\\"ConvertPath\\\":\\\"/Converted/ADLS/TripData/2022/01/25/16/\\\",\\\"SinkFolderPath\\\":\\\"Converted/ADLS/Files/Product/2022/01/07/15/DataValidation/\\\",\\\"FileName\\\":\\\"\\\",\\\"RowsRead\\\":null,\\\"RowsCopied\\\":null,\\\"SourceType\\\":\\\"parquet\\\",\\\"SchemaName\\\":\\\"Files\\\",\\\"SrcObject\\\":\\\"TripData_20130101.parquet\\\",\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/jsonfile\\\",\\\"IPAddress\\\":null,\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/Data Validation/NB_01_RowCount\\\",\\\"FileFormat\\\":\\\"parquet\\\",\\\"FunctionName\\\":\\\"RowCount\\\",\\\"DvMethod\\\":\\\"Databricks\\\",\\\"ConditionFlag\\\":1,\\\"EntRunId\\\":\\\"59f03a27-fa44-4c9b-92ec-598d4a808166\\\",\\\"InputParameter\\\":null}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SinkGlobalParameters =\"\"\r\n",
							"DataValidationParameters=\"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Get the necessary variables\r\n",
							"fwklog_id = dv_params_dict[\"FwkLogId\"]  \r\n",
							"function_name = str(dv_params_dict[\"FunctionName\"]).strip()                           # Validation Function name\r\n",
							"dv_method = str(dv_params_dict[\"DvMethod\"]).strip()                                   # Data validation method name\r\n",
							"\r\n",
							"\r\n",
							"source_path = dv_params_dict['ConvertPath']\r\n",
							"#replace // to get the correct path \r\n",
							"source_path = source_path.replace('//', '/')\r\n",
							"#adls_source_name = 'adlsmetadatadriven2.dfs.core.windows.net/'\r\n",
							"adls_source_name = adls_storage_account_name + '.dfs.core.windows.net/'\r\n",
							"\r\n",
							"source_container_name = 'sink'\r\n",
							"\r\n",
							"kv_scope_name = sink_params_dict[\"kv_scope_name\"]                                     # Name of the Azure Key Vault-backed scope\r\n",
							"kv_workspace_id = sink_params_dict[\"kv_workspace_id\"].strip()                         # Name of the secret for the log analytics workspace id\r\n",
							"kv_workspace_pk = sink_params_dict[\"kv_workspace_pk\"].strip()                         # Name of the secret for the log analytics primary key\r\n",
							"adls_storage_account_name = sink_params_dict[\"adls_storage_name\"].strip()    # Name of the Azure Blob Storage Account \r\n",
							"adls_blob_secret_name = sink_params_dict[\"adls_storage_secret_name\"].strip()            # Name of the container in the Azure Blob Storage \r\n",
							"\r\n",
							"#storage_account_name = adls_storage_account_name              # Name of the Azure Blob Storage Account we store it in the same as adls\r\n",
							"\r\n",
							"#condition = dv_params_dict[\"ConditionFlag\"]                                 # UPDATE ConditionFlag new value\r\n",
							"# Declare input parameter variable\r\n",
							"#column_names = input_parameter_dict[\"Column Name\"] if \"Column Name\" in input_parameter_dict else '' # Name(s) of column(s) to validate, there may be no parameters (for example NB_01) \r\n",
							"\r\n",
							"# Declare variables for NB_05_MinMaxRowCount\r\n",
							"min_value = input_parameter_dict[\"Min Value\"]                                 # Lower bound of the expected range of records\r\n",
							"print('min_value:', min_value)\r\n",
							"max_value = input_parameter_dict[\"Max Value\"]                                 # Upper bound of the expected range of records\r\n",
							"print('max_value:', max_value)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#read parquet file\r\n",
							"#connet to the Sink\r\n",
							" # Function to Access Azure Blob storage using the DataFrame API reads json, csv, parquet and xml file and writes it into parquet \r\n",
							"def read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path):\r\n",
							"  \r\n",
							"  \r\n",
							"  try:\r\n",
							"    \r\n",
							"    print(\"Start process\")\r\n",
							"     #Set up an account access keySet up an account access key\r\n",
							"    spark.conf.set(\"spark.storage.synapse.linkedServiceName\", kv_scope_name)\r\n",
							"    spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\r\n",
							"    spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"    \"\"\"Read data Output Files and create delta tables \"\"\"\r\n",
							"    path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"\r\n",
							"    print(\"******************************** {}\".format(f'{path}*.parquet'))\r\n",
							"    try:\r\n",
							"      \r\n",
							"      df = spark.read.parquet(f'{path}*.snappy.parquet')\r\n",
							"      \r\n",
							"    except:\r\n",
							"      \r\n",
							"      df = spark.read.parquet(f'{path}*.parquet')\r\n",
							"\r\n",
							"    \r\n",
							"    return df    \r\n",
							"  \r\n",
							"  except Exception as ex:\r\n",
							"    raise Exception(f'Error: {ex}')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def row_count(df):\r\n",
							"  rcnt = df.count()\r\n",
							"  return rcnt"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def Compare_Count_Files(sink_file_rows_count, min_value, max_value, fwklog_id, kv_scope_name, kv_workspace_id, kv_workspace_pk):\r\n",
							"  # Build the inputs for the DataValidationLog table\r\n",
							"  # If Min Value is defined as zero, null, or empty -> Min Value will be taken as zero.\r\n",
							"  if min_value <= max_value:\r\n",
							"    if isinstance(min_value, str):\r\n",
							"      if min_value.lower() == \"null\" or min_value == \"\":\r\n",
							"        min_value = 0\r\n",
							"      else: \r\n",
							"        min_value = int(min_value)\r\n",
							"\r\n",
							"    if isinstance(max_value, str):\r\n",
							"      if max_value.lower() == \"null\" or max_value == \"\":\r\n",
							"        max_value = 0\r\n",
							"      else:\r\n",
							"        max_value = int(max_value)\r\n",
							"\r\n",
							"    # Validate that the sink record count is inside the thresholds.\r\n",
							"    if max_value == 0:\r\n",
							"      if sink_file_rows_count >= min_value:\r\n",
							"        validation_status = \"Succeeded\"\r\n",
							"        validation_bool = \"True\"\r\n",
							"        message = f\"MinMaxRowCount was applied. The values are between {min_value} and {max_value}\"\r\n",
							"      else:\r\n",
							"        validation_status = \"Failed\"\r\n",
							"        validation_bool = \"False\"\r\n",
							"        message = \"MinMaxRowCount was applied. The number of records is not higher than {}. The actual number is: {}.\".format(str(min_value), str(sink_file_rows_count))\r\n",
							"    else:\r\n",
							"      if sink_file_rows_count >= min_value and sink_file_rows_count <= max_value:\r\n",
							"        validation_status = \"Succeeded\"\r\n",
							"        validation_bool = \"True\"\r\n",
							"        message = \"MinMaxRowCount was applied.\"\r\n",
							"      else:\r\n",
							"        validation_status = \"Failed\"\r\n",
							"        validation_bool = \"False\"\r\n",
							"        message = \"MinMaxRowCount was applied. The number of records is not between {} and {}. The actual number is: {}.\".format(str(min_value), str(max_value), str(sink_file_rows_count))\r\n",
							"  else:\r\n",
							"    validation_status = \"Failed\"\r\n",
							"    validation_bool = \"False\"\r\n",
							"    message = \"MinMaxRowCount was applied. The min value is greater than the max value.\"\r\n",
							"  \r\n",
							"  output = {'ExecutionStatus': 'Successfull',\"FwkLogId\": fwklog_id, \"Output\": {\"Count\": sink_file_rows_count, \"Validation\": { \"Status\": validation_bool, \"Message\": message}}}\r\n",
							"  \r\n",
							"  #print(output)\r\n",
							"  return output"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Main method for validation.\r\n",
							"def Validating_NB_05_MinMaxRowCount(df, min_value, max_value, fwklog_id, kv_scope_name, kv_workspace_id, kv_workspace_pk):\r\n",
							"  try:\r\n",
							"    #sink_file_rows_count = count_rows(sink_file)\r\n",
							"    sink_file_rows_count = row_count(df)\r\n",
							"    return Compare_Count_Files(sink_file_rows_count, min_value, max_value, fwklog_id, kv_scope_name, kv_workspace_id, kv_workspace_pk)\r\n",
							"  except Exception as error:\r\n",
							"    print(\"*** ERROR in Validating_NB_05_MinMaxRowCount:\", error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute main method for validation','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"    #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"    return {}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Comparing rows counts.\r\n",
							"try:\r\n",
							"  json_output = Validating_NB_05_MinMaxRowCount(df, min_value, max_value, fwklog_id, kv_scope_name, kv_workspace_id, kv_workspace_pk)\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, json_output)\r\n",
							"except Exception as err:\r\n",
							"  print(f\"An error has occurred: {err}\")\r\n",
							"  msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail at Comparing rows MinMaxRow counts','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Pass parameter to ADF\r\n",
							"mssparkutils.notebook.exit(json_output)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NB_06_SchemaDrift')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "MDMF_Validation"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "felitztapia",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "59169280-a1d5-4a27-94b7-08ffd1048e2d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
						"name": "felitztapia",
						"type": "Spark",
						"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Metadata-Driven Ingestion Framework \r\n",
							"#### Data Validation: 06 SchemaDrift\r\n",
							"Connect to sink instance, obtain the schema of the file ingested and compare it to a schema defined in a JSON file located in the same sink instance.\r\n",
							"This validation takes as input parameter the allow evolution boolean value."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### IMPORTANT!\r\n",
							"#### Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data for testing.\r\n",
							"SinkGlobalParameters =  \"{\\\"kv_scope_name\\\":\\\"ADLS_Spark\\\",\\\"kv_workspace_id\\\":\\\"MDIF-la-workspace-Id\\\",\\\"kv_workspace_pk\\\":\\\"MDIF-la-workspace-pk\\\",\\\"raw_storage_secret\\\":\\\"\\\",\\\"raw_storage_name\\\":\\\"\\\",\\\"sink_container_name\\\":\\\"sink\\\",\\\"schema_container_name\\\":\\\"schemas\\\",\\\"output_container_name\\\":\\\"datatransformation\\\",\\\"adls2_storage_account_name\\\":\\\"adlsmetadatadriven2\\\",\\\"adls2_blob_secret_name\\\":\\\"MDIF-ADLSmetadatadriven2-AccountKey\\\",\\\"sink_type\\\":\\\"ADLS\\\"}\"\r\n",
							"\r\n",
							"DataValidationParameters = \"{\\\"FwkLogId\\\":5,\\\"SrcObjectChild\\\":\\\"Product.csv\\\",\\\"DvMappingId\\\":11,\\\"SourcePath\\\":\\\"source/jsonfile/Product.json\\\",\\\"ConvertPath\\\":\\\"/Converted/ADLS/TripData/2022/01/25/16/\\\",\\\"SinkFolderPath\\\":\\\"Converted/ADLS/Files/Product/2022/01/07/15/DataValidation/\\\",\\\"FileName\\\":\\\"\\\",\\\"RowsRead\\\":null,\\\"RowsCopied\\\":null,\\\"SourceType\\\":\\\"parquet\\\",\\\"SchemaName\\\":\\\"Files\\\",\\\"SrcObject\\\":\\\"TripData_20130101.parquet\\\",\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/jsonfile\\\",\\\"IPAddress\\\":null,\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/Data Validation/NB_01_RowCount\\\",\\\"FileFormat\\\":\\\"parquet\\\",\\\"FunctionName\\\":\\\"RowCount\\\",\\\"DvMethod\\\":\\\"Databricks\\\",\\\"ConditionFlag\\\":1,\\\"EntRunId\\\":\\\"59f03a27-fa44-4c9b-92ec-598d4a808166\\\",\\\"InputParameter\\\":null}\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SinkGlobalParameters =\"\"\r\n",
							"DataValidationParameters=\"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Get the necessary variables\r\n",
							"fwklog_id = dv_params_dict[\"FwkLogId\"]  \r\n",
							"function_name = str(dv_params_dict[\"FunctionName\"]).strip()                           # Validation Function name\r\n",
							"dv_method = str(dv_params_dict[\"DvMethod\"]).strip()                                   # Data validation method name\r\n",
							"\r\n",
							"\r\n",
							"source_path = dv_params_dict['ConvertPath']\r\n",
							"#replace // to get the correct path \r\n",
							"source_path = source_path.replace('//', '/')\r\n",
							"#adls_source_name = 'adlsmetadatadriven2.dfs.core.windows.net/'\r\n",
							"adls_source_name = adls_storage_account_name + '.dfs.core.windows.net/'\r\n",
							"\r\n",
							"sink_type=sink_params_dict[\"sink_type\"]\r\n",
							"source_container_name = 'sink'\r\n",
							"\r\n",
							"kv_scope_name = sink_params_dict[\"kv_scope_name\"]                                     # Name of the Azure Key Vault-backed scope\r\n",
							"kv_workspace_id = sink_params_dict[\"kv_workspace_id\"].strip()                         # Name of the secret for the log analytics workspace id\r\n",
							"kv_workspace_pk = sink_params_dict[\"kv_workspace_pk\"].strip()                         # Name of the secret for the log analytics primary key\r\n",
							"adls_storage_account_name = sink_params_dict[\"adls_storage_name\"].strip()    # Name of the Azure Blob Storage Account \r\n",
							"adls_blob_secret_name = sink_params_dict[\"adls_storage_secret_name\"].strip()            # Name of the container in the Azure Blob Storage \r\n",
							"\r\n",
							"storage_account_name = adls_storage_account_name              # Name of the Azure Blob Storage Account we store it in the same as adls\r\n",
							"\r\n",
							"condition = dv_params_dict[\"ConditionFlag\"]                                 # UPDATE ConditionFlag new value\r\n",
							"# Declare input parameter variable\r\n",
							"column_names = input_parameter_dict[\"Column Name\"] if \"Column Name\" in input_parameter_dict else '' # Name(s) of column(s) to validate, there may be no parameters (for example NB_01) \r\n",
							"allow_evolution_bool = input_parameter_dict[\"Allow Evolution\"]     # Boolean parameter that allows schema drift\r\n",
							"\r\n",
							"# Declare variables for NB_06_SchemaDrift\r\n",
							"schema_file_name = dv_params_dict[\"SrcObjectChild\"].replace('.csv','.json').strip()   # Name of the schema file     \r\n",
							"schema_container_name = sink_params_dict[\"schema_container_name\"].strip()             # Name of the container of the schema file"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#read parquet file\r\n",
							"#connet to the Sink\r\n",
							" # Function to Access Azure Blob storage using the DataFrame API reads json, csv, parquet and xml file and writes it into parquet \r\n",
							"def read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path):\r\n",
							"  \r\n",
							"  \r\n",
							"  try:\r\n",
							"    \r\n",
							"    print(\"Start process\")\r\n",
							"     #Set up an account access keySet up an account access key\r\n",
							"    spark.conf.set(\"spark.storage.synapse.linkedServiceName\", kv_scope_name)\r\n",
							"    spark.conf.set(\"fs.azure.account.auth.type\", \"SAS\")\r\n",
							"    spark.conf.set(\"fs.azure.sas.token.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"    \"\"\"Read data Output Files and create delta tables \"\"\"\r\n",
							"    path = \"abfss://{}@{}{}\".format(source_container_name, adls_source_name, source_path)\r\n",
							"\r\n",
							"    print(\"******************************** {}\".format(f'{path}*.parquet'))\r\n",
							"    try:\r\n",
							"      df = spark.read.parquet(f'{path}*.snappy.parquet')\r\n",
							"    except:\r\n",
							"      df = spark.read.parquet(f'{path}*.parquet')\r\n",
							"\r\n",
							"    \r\n",
							"    return df    \r\n",
							"  \r\n",
							"  except Exception as ex:\r\n",
							"    raise Exception(f'Error: {ex}')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sink_file = read_pqt(kv_scope_name, adls_blob_secret_name, adls_storage_account_name, source_container_name, adls_source_name, source_path)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Function to join and separate columns from sink and schema files.\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"def compare_schema(sink_file_columns, schema_file_columns):\r\n",
							"  \r\n",
							"  print('\\n> Comparing columns sink vs schema:\\n>> Source Schema Sink:   {}\\n>> JSON Schema: {}'.format(sink_file_columns, schema_file_columns))\r\n",
							"  sink_file_columns = pd.DataFrame(sink_file_columns)\r\n",
							"  schema_file_columns =  pd.DataFrame(schema_file_columns)\r\n",
							"  merge = sink_file_columns.merge(schema_file_columns, how='outer', indicator=True)\r\n",
							"  del_cols = merge.query('_merge == \"left_only\"').drop('_merge', 1)\r\n",
							"  new_cols = merge.query('_merge == \"right_only\"').drop('_merge', 1)\r\n",
							"  del_cols_list = []\r\n",
							"  new_cols_list = []\r\n",
							"  \r\n",
							"  for item in del_cols[0]:\r\n",
							"    del_cols_list.append(item)\r\n",
							"    \r\n",
							"  for item in new_cols[0]:\r\n",
							"    new_cols_list.append(item)\r\n",
							"  \r\n",
							"  return del_cols_list, new_cols_list"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Build the inputs for the DvLog table\r\n",
							"def Build_Logs(del_cols_list, new_cols_list,fwklog_id,allow_evolution_bool):\r\n",
							"  \r\n",
							"  validation_bool = \"True\"\r\n",
							"\r\n",
							"  if new_cols_list is None:\r\n",
							"    count_drifted_values = 0\r\n",
							"  else: \r\n",
							"    count_drifted_values = len(del_cols_list) + len(new_cols_list)\r\n",
							"\r\n",
							"  if new_cols_list is None:\r\n",
							"    message = \"Schema file does not exist\"\r\n",
							"    validation_bool = \"False\"\r\n",
							"  elif del_cols_list != [] or new_cols_list != []:\r\n",
							"    message = \"Schema drift identified. \"\r\n",
							"    if del_cols_list != []:\r\n",
							"      message = message + \"Deleted columns: \" + str(del_cols_list) + \". \"\r\n",
							"      if allow_evolution_bool.lower() == \"false\":\r\n",
							"        validation_bool = \"False\"\r\n",
							"\r\n",
							"    if new_cols_list != []:\r\n",
							"      message = message + \"Added columns: \" + str(new_cols_list) + \".\"\r\n",
							"      if allow_evolution_bool.lower() == \"false\":\r\n",
							"        validation_bool = \"False\"\r\n",
							"  \r\n",
							"  json_output = {'ExecutionStatus': 'Successfull',\"FwkLogId\": fwklog_id, \"Output\": {\"Count\": str(count_drifted_values), \"Validation\": { \"Status\": validation_bool, \"Message\": message}}}\r\n",
							"  return json_output"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Main method for validation.\r\n",
							"def Validating_NB_06_SchemaDrift(schema_file_name, schema_container_name,storage_account_name, adls_storage_account_name,fwklog_id,function_name,dv_method,kv_scope_name,kv_workspace_id,kv_workspace_pk,allow_evolution_bool):\r\n",
							"  schema_file_columns = []\r\n",
							"  sink_file_columns = []\r\n",
							"  \r\n",
							"  try:\r\n",
							"    \"\"\"Read data from schema file\"\"\"\r\n",
							"    \r\n",
							"    if not schema_file_name.endswith(\".json\"):\r\n",
							"      schema_file_name += \".json\"\r\n",
							"    \r\n",
							"    if sink_type.upper() == \"BLOB\":\r\n",
							"      print('\\n> Getting schema file from BLOB.')\r\n",
							"      schema_file = spark.read.json(\"wasbs://{}@{}.blob.core.windows.net/{}\".format(schema_container_name, storage_account_name, schema_file_name))\r\n",
							"    elif sink_type.upper() == \"ADLS\":\r\n",
							"      print('\\n> Getting schema file from ADLS.')\r\n",
							"      schema_file = spark.read.json(\"abfss://{}@{}.dfs.core.windows.net/{}\".format(schema_container_name, adls_storage_account_name, schema_file_name))      \r\n",
							"    else:\r\n",
							"      print(\"\\n> PLEASE CHOOSE BETWEEN BLOB OR ADLS\")\r\n",
							"      \r\n",
							"    schema_file_columns = schema_file.columns #Schema file\r\n",
							"    sink_file_columns = sink_file.columns  #Original file in sink\r\n",
							"    \r\n",
							"  except Exception as error:\r\n",
							"    schema_file = None\r\n",
							"    schema_file_columns = None\r\n",
							"    print(\"*** ERROR:\", error)\r\n",
							"    msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail to execute main method for validation','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"    #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"    \r\n",
							"  if sink_file != None and schema_file != None:\r\n",
							"    print(\"> Successfull sink and schema file loaded\")\r\n",
							"    # Compare columns and obtain the added and deleted columns\r\n",
							"    del_cols, new_cols = compare_schema(sink_file_columns, schema_file_columns)\r\n",
							"  else:\r\n",
							"    print(\"> Unsuccessfull to load sink and schema files, please see the logs\")\r\n",
							"    del_cols, new_cols = sink_file_columns, schema_file_columns\r\n",
							"    \r\n",
							"  return Build_Logs(del_cols, new_cols,fwklog_id,allow_evolution_bool)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Getting schema file and comparing with sink file.\r\n",
							"try:\r\n",
							"  \r\n",
							"  json_output = Validating_NB_06_SchemaDrift(schema_file_name, schema_container_name,storage_account_name, adls_storage_account_name,fwklog_id,function_name,dv_method,kv_scope_name,kv_workspace_id,kv_workspace_pk,allow_evolution_bool)\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, json_output)\r\n",
							"except Exception as err:\r\n",
							"  print(f\"An error has occurred: {err}\")\r\n",
							"  msg_error = {'ExecutionStatus': 'Failed','Error Message':'Fail at Getting schema file and comparing with sink file','FwkLogId': fwklog_id,'FunctionName': function_name ,'DvMethod':dv_method}\r\n",
							"  #post_data(kv_scope_name, kv_workspace_id, kv_workspace_pk, msg_error)\r\n",
							"  \r\n",
							"  \r\n",
							"json_output"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Pass parameter to ADF\r\n",
							"mssparkutils.notebook.exit(json_output)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ReadingParameters')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "94e29f4e-2c61-45fb-86a5-53be0970c80a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**IMPORTANT**\r\n",
							"Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"testing = False #<------ IMPORTANT!: Change the value of \"testing=False\" for production environment.\r\n",
							"print_dictionaries = False\r\n",
							"print_common_variables = False\r\n",
							"print_empty_variables = False"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"SinkGlobalParameters =\"\"\r\n",
							"DataValidationParameters=\"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert string (json) parameters to dictionaries.\r\n",
							"import json\r\n",
							"\r\n",
							"dv_params_dict = json.loads(DataValidationParameters)\r\n",
							"sink_params_dict = json.loads(SinkGlobalParameters)\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(dv_params_dict)\r\n",
							"print(sink_params_dict)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.notebook.exit({\"SinkGlobalParameters\":sink_params_dict,\"DataValidationParameters\":dv_params_dict })"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testingFTR')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Metadata-Driven Ingestion Framework \nData Validation: RowCount\nConnect to source and sink instances and count the rows from the file copied. Validate the result and send it to Azure Data Factory.",
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "felitztapia",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "1",
						"spark.autotune.trackingId": "6f44ee89-ddfa-4473-84df-95b377fd368d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/28100aed-fbc9-43b5-be58-aebd70043c6b/resourceGroups/MDC-Felix-RG/providers/Microsoft.Synapse/workspaces/ftr23m5xeeoklumapocws1/bigDataPools/felitztapia",
						"name": "felitztapia",
						"type": "Spark",
						"endpoint": "https://ftr23m5xeeoklumapocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/felitztapia",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**IMPORTANT**\r\n",
							"Configuration for testing and debug\r\n",
							"Change the value of \"testing=False\" for production environment.\r\n",
							"Change the value of debug variables to see or hide prints with information"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Data for testing.\r\n",
							"SinkGlobalParameters =  \"{\\\"kv_scope_name\\\":\\\"ADLS_Spark\\\",\\\"kv_workspace_id\\\":\\\"MDIF-la-workspace-Id\\\",\\\"kv_workspace_pk\\\":\\\"MDIF-la-workspace-pk\\\",\\\"raw_storage_secret\\\":\\\"\\\",\\\"raw_storage_name\\\":\\\"\\\",\\\"sink_container_name\\\":\\\"synapsedata\\\",\\\"schema_container_name\\\":\\\"schemas\\\",\\\"output_container_name\\\":\\\"datatransformation\\\",\\\"adls_storage_name\\\":\\\"aicscpgdemo02\\\",\\\"adls_storage_secret_name\\\":\\\"MDIF-ADLSmetadatadriven2-AccountKey\\\",\\\"sink_type\\\":\\\"ADLS\\\"}\"\r\n",
							"\r\n",
							"DataValidationParameters = \"{\\\"FwkLogId\\\":5,\\\"SrcObjectChild\\\":\\\"Product.csv\\\",\\\"DvMappingId\\\":11,\\\"SourcePath\\\":\\\"source/jsonfile/Product.json\\\",\\\"ConvertPath\\\":\\\"/Converted/ADLS/TripData/2022/01/25/16/\\\",\\\"SinkFolderPath\\\":\\\"Converted/ADLS/Files/Product/2022/01/07/15/DataValidation/\\\",\\\"FileName\\\":\\\"\\\",\\\"RowsRead\\\":null,\\\"RowsCopied\\\":null,\\\"SourceType\\\":\\\"parquet\\\",\\\"SchemaName\\\":\\\"Files\\\",\\\"SrcObject\\\":\\\"TripData_20130101.parquet\\\",\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/jsonfile\\\",\\\"IPAddress\\\":null,\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/Data Validation/NB_01_RowCount\\\",\\\"FileFormat\\\":\\\"parquet\\\",\\\"FunctionName\\\":\\\"RowCount\\\",\\\"DvMethod\\\":\\\"Databricks\\\",\\\"ConditionFlag\\\":1,\\\"EntRunId\\\":\\\"59f03a27-fa44-4c9b-92ec-598d4a808166\\\",\\\"InputParameter\\\": {\\\"Column Name\\\":\\\"DateId\\\"}}\"\r\n",
							"DataValidationParameters = \"{\\\"FwkLogId\\\":5,\\\"SrcObjectChild\\\":\\\"Product.csv\\\",\\\"DvMappingId\\\":11,\\\"SourcePath\\\":\\\"source/jsonfile/Product.json\\\",\\\"ConvertPath\\\":\\\"/Converted/ADLS/TripData/2022/01/25/16/\\\",\\\"SinkFolderPath\\\":\\\"Converted/ADLS/Files/Product/2022/01/07/15/DataValidation/\\\",\\\"FileName\\\":\\\"\\\",\\\"RowsRead\\\":null,\\\"RowsCopied\\\":null,\\\"SourceType\\\":\\\"parquet\\\",\\\"SchemaName\\\":\\\"Files\\\",\\\"SrcObject\\\":\\\"TripData_20130101.parquet\\\",\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/jsonfile\\\",\\\"IPAddress\\\":null,\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/Data Validation/NB_01_RowCount\\\",\\\"FileFormat\\\":\\\"parquet\\\",\\\"FunctionName\\\":\\\"RowCount\\\",\\\"DvMethod\\\":\\\"Databricks\\\",\\\"ConditionFlag\\\":1,\\\"EntRunId\\\":\\\"59f03a27-fa44-4c9b-92ec-598d4a808166\\\",\\\"InputParameter\\\": {\\\"DateId\\\":[\\\"[0-9]{5}\\\"]}}\"\r\n",
							"DataValidationParameters = \"{\\\"FwkLogId\\\":5,\\\"SrcObjectChild\\\":\\\"Product.csv\\\",\\\"DvMappingId\\\":11,\\\"SourcePath\\\":\\\"source/jsonfile/Product.json\\\",\\\"ConvertPath\\\":\\\"/Converted/ADLS/TripData/2022/01/25/16/\\\",\\\"SinkFolderPath\\\":\\\"Converted/ADLS/Files/Product/2022/01/07/15/DataValidation/\\\",\\\"FileName\\\":\\\"\\\",\\\"RowsRead\\\":null,\\\"RowsCopied\\\":null,\\\"SourceType\\\":\\\"parquet\\\",\\\"SchemaName\\\":\\\"Files\\\",\\\"SrcObject\\\":\\\"TripData_20130101.parquet\\\",\\\"InstanceURL\\\":\\\"https://adlsmetadatadriven.dfs.core.windows.net/\\\",\\\"Port\\\":null,\\\"UserName\\\":null,\\\"SecretName\\\":\\\"MDIF-ADLS-AccountKey\\\",\\\"SrcPath\\\":\\\"source/jsonfile\\\",\\\"IPAddress\\\":null,\\\"NotebookPath\\\":\\\"/Shared/Metadata Driven Ingestion Framework/Data Validation/NB_01_RowCount\\\",\\\"FileFormat\\\":\\\"parquet\\\",\\\"FunctionName\\\":\\\"RowCount\\\",\\\"DvMethod\\\":\\\"Databricks\\\",\\\"ConditionFlag\\\":1,\\\"EntRunId\\\":\\\"59f03a27-fa44-4c9b-92ec-598d4a808166\\\",\\\"InputParameter\\\": {\\\"Target Count\\\":\\\"10\\\"}}\""
						],
						"outputs": [],
						"execution_count": 126
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": true
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"json_output=\"\"\r\n",
							"json_output=mssparkutils.notebook.run('NB_04_RecordLevel',600, {'SinkGlobalParameters':SinkGlobalParameters,'DataValidationParameters':DataValidationParameters})"
						],
						"outputs": [],
						"execution_count": 127
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"# Pass parameter to ADF\r\n",
							"mssparkutils.notebook.exit(json_output)"
						],
						"outputs": [],
						"execution_count": 124
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import json\r\n",
							"\r\n",
							"dv_params_dict = json.loads(DataValidationParameters)\r\n",
							"\r\n",
							"sink_params_dict = json.loads(SinkGlobalParameters)"
						],
						"outputs": [],
						"execution_count": 114
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"input_parameter_dict=dv_params_dict['InputParameter']"
						],
						"outputs": [],
						"execution_count": 119
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import numpy\r\n",
							"column_names = numpy.array(list(input_parameter_dict.keys())) "
						],
						"outputs": [],
						"execution_count": 120
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(column_names)"
						],
						"outputs": [],
						"execution_count": 121
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"column_names = (dv_params_dict['InputParameter']['Column Name']).split(',')\r\n",
							"print(column_names)"
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Declare input parameter variable\r\n",
							"column_names = (dv_params_dict['InputParameter']['Column Name']).split(',')\r\n",
							"#column_names = [columns.lower() for columns in column_names]\r\n",
							"column_names"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(dv_params_dict)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ftr23m5xeeoklumapocws1p1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southcentralus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		}
	]
}